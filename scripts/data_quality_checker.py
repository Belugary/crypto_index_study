#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡æ£€æŸ¥å’Œä¿®å¤å·¥å…·
æ£€æŸ¥æ‰€æœ‰å¸ç§æ–‡ä»¶çš„æ•°æ®å®Œæ•´æ€§ï¼Œè¯†åˆ«å¹¶ä¿®å¤æœ‰é—®é¢˜çš„æ–‡ä»¶
"""

import logging
import os
import sys
from datetime import datetime, timedelta

import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.updaters.price_updater import PriceDataUpdater

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")


class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""

    def __init__(self):
        self.data_dir = "data/coins"
        # ç§»é™¤PriceDataUpdaterçš„åˆå§‹åŒ–ï¼Œåªåœ¨éœ€è¦æ—¶åˆ›å»º
        self.updater = None
        self.min_rows = 100  # é™ä½æœ€å°è¡Œæ•°é˜ˆå€¼ï¼Œå¾ˆå¤šæ–°å¸ç§æ•°æ®ç¡®å®ä¸é•¿
        self.max_days_old = 2  # æ•°æ®æœ€å¤§è¿‡æœŸå¤©æ•°
        self.min_data_span_days = 30  # æœ€å°æ•°æ®è·¨åº¦å¤©æ•°ï¼Œç”¨äºåŒºåˆ†æ–°å¸ç§å’ŒçœŸæ­£è¿‡æœŸçš„æ•°æ®

    def _get_updater(self):
        """å»¶è¿Ÿåˆå§‹åŒ–updater"""
        if self.updater is None:
            self.updater = PriceDataUpdater()
        return self.updater

    def _is_data_recent(self, data_span_days, days_since_latest):
        """
        æ™ºèƒ½åˆ¤æ–­æ•°æ®æ˜¯å¦"æœ€æ–°"

        é€»è¾‘ï¼š
        1. å¦‚æœæ•°æ®è·¨åº¦å¾ˆçŸ­ï¼ˆ<30å¤©ï¼‰ï¼Œå¯èƒ½æ˜¯æ–°å¸ç§ï¼Œåªè¦ä¸è¶…è¿‡7å¤©å°±ç®—æ­£å¸¸
        2. å¦‚æœæ•°æ®è·¨åº¦é•¿ï¼ˆ>=30å¤©ï¼‰ï¼Œè¯´æ˜æ˜¯è€å¸ç§ï¼ŒæŒ‰æ ‡å‡†çš„2å¤©åˆ¤æ–­
        3. è¿™æ ·å¯ä»¥é¿å…æŠŠ"æ•°æ®æœ¬èº«å°±ä¸é•¿çš„æ–°å¸ç§"è¯¯åˆ¤ä¸º"æ•°æ®è¿‡æœŸ"

        Args:
            data_span_days: æ•°æ®è·¨åº¦å¤©æ•°ï¼ˆæœ€æ–°æ—¥æœŸ - æœ€æ—©æ—¥æœŸï¼‰
            days_since_latest: è·ç¦»æœ€æ–°æ•°æ®çš„å¤©æ•°

        Returns:
            bool: æ•°æ®æ˜¯å¦ç®—"æœ€æ–°"
        """
        if data_span_days < self.min_data_span_days:
            # æ–°å¸ç§æˆ–æ•°æ®è·¨åº¦çŸ­çš„å¸ç§ï¼Œç»™æ›´å®½æ¾çš„æ ‡å‡†
            return days_since_latest <= 7
        else:
            # è€å¸ç§ï¼Œä½¿ç”¨æ ‡å‡†çš„åˆ¤æ–­
            return days_since_latest <= self.max_days_old

    def check_timestamp_intervals(self, df, time_column):
        """æ£€æŸ¥æ—¶é—´æˆ³é—´éš”æ˜¯å¦åˆç†ï¼ˆä¸»è¦æ£€æµ‹æ˜¯å¦æœ‰1å¤©é—´éš”çš„ç¼ºå¤±ï¼‰"""
        try:
            if time_column == "timestamp":
                # è½¬æ¢æ¯«ç§’æ—¶é—´æˆ³ä¸ºæ—¥æœŸ
                dates = pd.to_datetime(df["timestamp"], unit="ms").dt.date
            else:
                # dateåˆ—ç›´æ¥ä½¿ç”¨
                dates = pd.to_datetime(df[time_column]).dt.date

            # å»é‡å¹¶æ’åº
            unique_dates = sorted(set(dates))

            if len(unique_dates) < 2:
                return True, "æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•æ£€æŸ¥é—´éš”"

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„é—´éš”ï¼ˆè¶…è¿‡7å¤©çš„ç¼ºå¤±ï¼‰
            large_gaps = []
            for i in range(1, len(unique_dates)):
                gap_days = (unique_dates[i] - unique_dates[i - 1]).days
                if gap_days > 7:  # è¶…è¿‡7å¤©çš„ç¼ºå¤±è¢«è®¤ä¸ºæ˜¯é—®é¢˜
                    large_gaps.append(
                        f"{unique_dates[i-1]} -> {unique_dates[i]} ({gap_days}å¤©)"
                    )

            if large_gaps:
                gap_info = "; ".join(large_gaps[:3])  # åªæ˜¾ç¤ºå‰3ä¸ª
                if len(large_gaps) > 3:
                    gap_info += f" ç­‰{len(large_gaps)}ä¸ªç¼ºå¤±"
                return False, f"å‘ç°å¤§æ—¶é—´ç¼ºå¤±: {gap_info}"

            return True, "æ—¶é—´é—´éš”æ­£å¸¸"

        except Exception as e:
            return True, f"æ—¶é—´é—´éš”æ£€æŸ¥å¤±è´¥: {str(e)}"

    def check_file_quality(self, filepath):
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
        try:
            df = pd.read_csv(filepath)
            row_count = len(df)

            # æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´ - æ”¯æŒdateåˆ—å’Œtimestampåˆ—
            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"]).dt.date
                latest_date = df["date"].max()
                earliest_date = df["date"].min()

                # è®¡ç®—æ•°æ®å¤©æ•°å’Œæœ€æ–°ç¨‹åº¦
                data_span_days = (latest_date - earliest_date).days
                days_since_latest = (datetime.now().date() - latest_date).days

                # æ£€æŸ¥æ—¶é—´æˆ³é—´éš”
                interval_ok, interval_msg = self.check_timestamp_intervals(df, "date")

            elif "timestamp" in df.columns:
                # å¤„ç†timestampåˆ—ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
                df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
                latest_date = df["datetime"].dt.date.max()
                earliest_date = df["datetime"].dt.date.min()

                # è®¡ç®—æ•°æ®å¤©æ•°å’Œæœ€æ–°ç¨‹åº¦
                data_span_days = (latest_date - earliest_date).days
                days_since_latest = (datetime.now().date() - latest_date).days

                # æ£€æŸ¥æ—¶é—´æˆ³é—´éš”
                interval_ok, interval_msg = self.check_timestamp_intervals(
                    df, "timestamp"
                )

            else:
                # æ²¡æœ‰æ—¶é—´åˆ—çš„æƒ…å†µ
                return {
                    "rows": row_count,
                    "latest_date": None,
                    "earliest_date": None,
                    "data_span_days": 0,
                    "days_since_latest": 999,
                    "is_recent": False,
                    "has_enough_data": row_count >= self.min_rows,
                    "interval_ok": False,
                    "interval_msg": "æ— æ—¶é—´åˆ—",
                }

            return {
                "rows": row_count,
                "latest_date": latest_date,
                "earliest_date": earliest_date,
                "data_span_days": data_span_days,
                "days_since_latest": days_since_latest,
                "is_recent": self._is_data_recent(data_span_days, days_since_latest),
                "has_enough_data": row_count >= self.min_rows,
                "interval_ok": interval_ok,
                "interval_msg": interval_msg,
            }
        except Exception as e:
            return {
                "error": str(e),
                "rows": 0,
                "is_recent": False,
                "has_enough_data": False,
                "interval_ok": False,
                "interval_msg": f"æ£€æŸ¥å¤±è´¥: {str(e)}",
            }

    def scan_all_files(self):
        """æ‰«ææ‰€æœ‰æ–‡ä»¶å¹¶åˆ†ç±»"""
        if not os.path.exists(self.data_dir):
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return

        files = [f for f in os.listdir(self.data_dir) if f.endswith(".csv")]

        print(f"ğŸ” æ‰«æ {len(files)} ä¸ªå¸ç§æ–‡ä»¶...")
        print("=" * 80)

        good_files = []
        problematic_files = []

        for i, filename in enumerate(files):
            # æ·»åŠ è¿›åº¦æ˜¾ç¤º
            if i % 50 == 0:
                print(f"è¿›åº¦: {i}/{len(files)} ({i/len(files)*100:.1f}%)")

            filepath = os.path.join(self.data_dir, filename)
            coin_name = filename[:-4]

            try:
                # æ·»åŠ å½“å‰å¤„ç†æ–‡ä»¶çš„è°ƒè¯•ä¿¡æ¯
                if i % 100 == 0:  # æ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡
                    print(f"æ­£åœ¨å¤„ç†: {coin_name}")

                quality = self.check_file_quality(filepath)

                try:
                    if "error" in quality:
                        print(f"âŒ {coin_name}: è¯»å–é”™è¯¯ - {quality['error']}")
                        problematic_files.append((coin_name, quality, "READ_ERROR"))
                    elif not quality["has_enough_data"]:
                        print(f"âš ï¸  {coin_name}: æ•°æ®ä¸è¶³ - {quality['rows']}è¡Œ")
                        problematic_files.append(
                            (coin_name, quality, "INSUFFICIENT_DATA")
                        )
                    elif not quality["interval_ok"]:
                        print(
                            f"âš ï¸  {coin_name}: æ—¶é—´é—´éš”å¼‚å¸¸ - {quality['interval_msg']}"
                        )
                        problematic_files.append((coin_name, quality, "INTERVAL_ISSUE"))
                    elif not quality["is_recent"]:
                        print(
                            f"âš ï¸  {coin_name}: æ•°æ®è¿‡æœŸ - æœ€æ–°:{quality['latest_date']} ({quality['days_since_latest']}å¤©å‰)"
                        )
                        problematic_files.append((coin_name, quality, "OUTDATED_DATA"))
                    else:
                        print(
                            f"âœ… {coin_name}: æ­£å¸¸ - {quality['rows']}è¡Œ, æœ€æ–°:{quality['latest_date']}"
                        )
                        good_files.append((coin_name, quality))
                except BrokenPipeError:
                    # ç®¡é“ä¸­æ–­æ—¶é™é»˜é€€å‡º
                    break
                except KeyboardInterrupt:
                    print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
                    break
            except Exception as e:
                print(f"âŒ {coin_name}: å¤„ç†å¼‚å¸¸ - {str(e)}")
                problematic_files.append(
                    (coin_name, {"error": str(e)}, "PROCESSING_ERROR")
                )
                continue

        try:
            print("\n" + "=" * 80)
            print(f"ğŸ“Š æ‰«æç»“æœ:")
            print(f"   âœ… æ­£å¸¸æ–‡ä»¶: {len(good_files)}")
            print(f"   âš ï¸  é—®é¢˜æ–‡ä»¶: {len(problematic_files)}")
        except BrokenPipeError:
            pass  # é™é»˜å¤„ç†ç®¡é“ä¸­æ–­

        return good_files, problematic_files

    def fix_problematic_files(self, problematic_files, dry_run=True):
        """ä¿®å¤æœ‰é—®é¢˜çš„æ–‡ä»¶"""
        if not problematic_files:
            print("ğŸ‰ æ²¡æœ‰å‘ç°é—®é¢˜æ–‡ä»¶éœ€è¦ä¿®å¤ï¼")
            return

        if dry_run:
            print(f"\nğŸ” DRY RUN: å°†ä¿®å¤ä»¥ä¸‹ {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶:")
        else:
            print(f"\nğŸ”§ å¼€å§‹ä¿®å¤ {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶:")

        for i, (coin_name, quality, issue_type) in enumerate(problematic_files, 1):
            print(f"\n[{i}/{len(problematic_files)}] å¤„ç† {coin_name} ({issue_type})")

            if dry_run:
                print(f"   ğŸ“‹ å°†æ‰§è¡Œ: é‡æ–°ä¸‹è½½å®Œæ•´å†å²æ•°æ®")
            else:
                try:
                    print(f"   ğŸ“¥ é‡æ–°ä¸‹è½½å®Œæ•´å†å²æ•°æ®...")
                    success, api_called = self._get_updater().download_coin_data(
                        coin_name
                    )

                    if success:
                        # é‡æ–°æ£€æŸ¥è´¨é‡
                        filepath = os.path.join(self.data_dir, f"{coin_name}.csv")
                        new_quality = self.check_file_quality(filepath)
                        print(
                            f"   âœ… ä¿®å¤æˆåŠŸ: {new_quality['rows']}è¡Œ, æœ€æ–°:{new_quality['latest_date']}"
                        )
                    else:
                        print(f"   âŒ ä¿®å¤å¤±è´¥")

                except Exception as e:
                    print(f"   âŒ ä¿®å¤é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    checker = DataQualityChecker()

    print("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·")
    print("=" * 50)  # æ‰«ææ‰€æœ‰æ–‡ä»¶
    result = checker.scan_all_files()
    if result is None:
        return

    good_files, problematic_files = result

    if problematic_files:
        print(f"\nâš ï¸  å‘ç° {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶:")
        for coin_name, quality, issue_type in problematic_files:
            if issue_type == "INSUFFICIENT_DATA":
                print(f"   ğŸ“‰ {coin_name}: ä»…{quality['rows']}è¡Œæ•°æ®")
            elif issue_type == "OUTDATED_DATA":
                print(f"   ğŸ“… {coin_name}: {quality['days_since_latest']}å¤©æœªæ›´æ–°")
            elif issue_type == "READ_ERROR":
                print(f"   ğŸ’¥ {coin_name}: æ–‡ä»¶è¯»å–é”™è¯¯")

        # è¯¢é—®æ˜¯å¦ä¿®å¤
        response = input(f"\nğŸ”§ æ˜¯å¦ä¿®å¤è¿™äº›é—®é¢˜æ–‡ä»¶? (y/N): ").strip().lower()

        if response == "y":
            checker.fix_problematic_files(problematic_files, dry_run=False)
        else:
            print("ğŸ“‹ è·³è¿‡ä¿®å¤ï¼Œæ‚¨å¯ä»¥ç¨åè¿è¡Œæ­¤å·¥å…·è¿›è¡Œä¿®å¤")
    else:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶æ•°æ®è´¨é‡è‰¯å¥½ï¼")


if __name__ == "__main__":
    main()
