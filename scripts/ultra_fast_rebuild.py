#!/usr/bin/env python3
"""
è¶…é«˜æ€§èƒ½æ¯æ—¥æ•°æ®é‡å»ºè„šæœ¬

ä¸“ä¸ºé«˜æ€§èƒ½ç³»ç»Ÿä¼˜åŒ–çš„é‡å»ºè„šæœ¬ï¼Œé‡‡ç”¨ä»¥ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼š
1. æ‰¹é‡å¤„ç†æ—¥æœŸä»¥å‡å°‘çº¿ç¨‹åˆ›å»ºå¼€é”€
2. å†…å­˜æ± åŒ–é¢„åŠ è½½æ•°æ®
3. ä¼˜åŒ–çš„I/Oæ“ä½œ
4. æ™ºèƒ½çº¿ç¨‹æ•°é…ç½®
"""

import argparse
import logging
import os
import sys
import multiprocessing
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

import pandas as pd
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler("logs/ultra_fast_rebuild.log"),
        ],
    )


class UltraFastDailyAggregator:
    """è¶…é«˜æ€§èƒ½æ¯æ—¥æ•°æ®æ±‡æ€»å™¨"""

    def __init__(
        self,
        coins_dir: str = "data/coins",
        output_dir: str = "data/daily/daily_files",
        batch_size: int = 100,  # æ‰¹å¤„ç†å¤§å°
    ):
        """
        åˆå§‹åŒ–æ±‡æ€»å™¨

        Args:
            coins_dir: å¸ç§æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œæ¯æ‰¹å¤„ç†å¤šå°‘å¤©
        """
        self.coins_dir = Path(coins_dir)
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.logger = logging.getLogger(__name__)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _load_coin_data_optimized(self, coin_id: str) -> Optional[pd.DataFrame]:
        """
        ä¼˜åŒ–çš„å¸ç§æ•°æ®åŠ è½½
        """
        csv_path = self.coins_dir / f"{coin_id}.csv"
        if not csv_path.exists():
            return None

        try:
            # è¯»å–æ•°æ®å¹¶åœ¨åç»­å¤„ç†ä¸­ä¼˜åŒ–æ•°æ®ç±»å‹
            df = pd.read_csv(csv_path)
            # é¢„å…ˆè®¡ç®—æ—¥æœŸï¼Œé¿å…é‡å¤è®¡ç®—
            df["date"] = pd.to_datetime(df["timestamp"], unit="ms").dt.date
            df["coin_id"] = coin_id
            return df
        except Exception as e:
            self.logger.warning(f"è¯»å– {coin_id} æ•°æ®å¤±è´¥: {e}")
            return None

    def _get_all_coin_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰å¸ç§ID"""
        coin_ids = []
        for csv_file in self.coins_dir.glob("*.csv"):
            coin_ids.append(csv_file.stem)
        return sorted(coin_ids)

    def _get_date_range_fast(self, coin_ids: List[str]) -> tuple[date, date]:
        """
        å¿«é€Ÿè·å–æ—¥æœŸèŒƒå›´ - ä½¿ç”¨é‡‡æ ·æ–¹æ³•
        """
        min_date = date.max
        max_date = date.min

        # åªé‡‡æ ·ä¸€éƒ¨åˆ†æ–‡ä»¶æ¥ç¡®å®šæ—¥æœŸèŒƒå›´ï¼Œå‡è®¾æ‰€æœ‰æ–‡ä»¶æ—¥æœŸèŒƒå›´ç›¸ä¼¼
        sample_size = min(50, len(coin_ids))
        sample_files = [
            self.coins_dir / f"{coin_ids[i]}.csv"
            for i in range(0, len(coin_ids), len(coin_ids) // sample_size)
        ]

        self.logger.info(f"å¿«é€Ÿæ‰«æ {len(sample_files)} ä¸ªæ ·æœ¬æ–‡ä»¶ä»¥ç¡®å®šæ—¥æœŸèŒƒå›´...")

        for csv_file in tqdm(sample_files, desc="å¿«é€Ÿæ‰«ææ—¥æœŸèŒƒå›´"):
            try:
                # åªè¯»å–æ—¶é—´æˆ³åˆ—
                df = pd.read_csv(
                    csv_file, usecols=["timestamp"], dtype={"timestamp": "int64"}
                )
                if df.empty:
                    continue

                # å¿«é€Ÿè·å–æœ€å°å’Œæœ€å¤§æ—¶é—´æˆ³
                min_ts = df["timestamp"].min()
                max_ts = df["timestamp"].max()

                current_min = pd.to_datetime(min_ts, unit="ms").date()
                current_max = pd.to_datetime(max_ts, unit="ms").date()

                if current_min < min_date:
                    min_date = current_min
                if current_max > max_date:
                    max_date = current_max

            except Exception as e:
                self.logger.warning(f"å¿«é€Ÿæ‰«æ {csv_file.stem} å¤±è´¥: {e}")
                continue

        if min_date == date.max or max_date == date.min:
            raise ValueError("æ— æ³•ä»æ ·æœ¬æ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´")

        return min_date, max_date

    def _process_date_batch(
        self, date_batch: List[date], all_coin_data: Dict[str, pd.DataFrame]
    ) -> int:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªæ—¥æœŸ

        Returns:
            æˆåŠŸå¤„ç†çš„æ—¥æœŸæ•°é‡
        """
        success_count = 0

        for target_date in date_batch:
            try:
                # æ±‡æ€»å½“æ—¥æ•°æ®
                daily_df = self._aggregate_daily_data(target_date, all_coin_data)

                if not daily_df.empty:
                    if self._save_daily_file(daily_df, target_date):
                        success_count += 1
                else:
                    success_count += 1  # å³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿç®—æˆåŠŸå¤„ç†

            except Exception as e:
                self.logger.error(f"æ‰¹å¤„ç† {target_date} å¤±è´¥: {e}")

        return success_count

    def _aggregate_daily_data(
        self, target_date: date, all_coin_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """
        æ±‡æ€»æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        """
        daily_data = []

        for coin_id, df in all_coin_data.items():
            if df is None:
                continue

            try:
                # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæŸ¥æ‰¾ç›®æ ‡æ—¥æœŸçš„æ•°æ®
                target_mask = df["date"] == target_date
                target_data = df[target_mask]

                if target_data.empty:
                    continue

                # å–æœ€æ–°çš„è®°å½•
                latest_record = target_data.iloc[-1]

                # å¿«é€Ÿæœ‰æ•ˆæ€§æ£€æŸ¥
                if (
                    pd.notna(latest_record["price"])
                    and latest_record["price"] > 0
                    and pd.notna(latest_record["market_cap"])
                    and latest_record["market_cap"] > 0
                ):
                    daily_data.append(
                        {
                            "timestamp": int(latest_record["timestamp"]),
                            "price": float(latest_record["price"]),
                            "volume": (
                                float(latest_record["volume"])
                                if pd.notna(latest_record["volume"])
                                else 0.0
                            ),
                            "market_cap": float(latest_record["market_cap"]),
                            "date": target_date.strftime("%Y-%m-%d"),
                            "coin_id": coin_id,
                        }
                    )

            except Exception as e:
                continue  # é™é»˜è·³è¿‡é”™è¯¯

        if not daily_data:
            return pd.DataFrame()

        # ä½¿ç”¨é«˜æ•ˆçš„DataFrameåˆ›å»º
        df = pd.DataFrame(daily_data)

        # ä¼˜åŒ–æ’åºå’Œæ’åæ“ä½œ
        df = df.sort_values("market_cap", ascending=False, ignore_index=True)
        df["rank"] = range(1, len(df) + 1)

        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        df = df[
            ["timestamp", "price", "volume", "market_cap", "date", "coin_id", "rank"]
        ]

        return df

    def _save_daily_file(self, df: pd.DataFrame, target_date: date) -> bool:
        """
        ä¼˜åŒ–çš„ä¿å­˜æ¯æ—¥æ•°æ®æ–‡ä»¶
        """
        if df.empty:
            return False

        # åˆ›å»ºå¹´æœˆç›®å½•
        year_dir = self.output_dir / str(target_date.year)
        month_dir = year_dir / f"{target_date.month:02d}"
        month_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        filename = f"{target_date}.csv"
        filepath = month_dir / filename

        try:
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„ä¿å­˜é€‰é¡¹
            df.to_csv(
                filepath, index=False, float_format="%.6f", lineterminator="\n"
            )  # ä½¿ç”¨Unixæ¢è¡Œç¬¦
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜ {target_date} æ•°æ®å¤±è´¥: {e}")
            return False

    def ultra_fast_rebuild(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        max_workers: Optional[int] = None,
    ) -> None:
        """
        è¶…é«˜æ€§èƒ½é‡å»ºæ‰€æœ‰æ¯æ—¥æ–‡ä»¶
        """
        self.logger.info("ğŸš€ å¯åŠ¨è¶…é«˜æ€§èƒ½é‡å»ºæ¨¡å¼")

        # è·å–æ‰€æœ‰å¸ç§
        coin_ids = self._get_all_coin_ids()
        self.logger.info(f"æ‰¾åˆ° {len(coin_ids)} ä¸ªå¸ç§")

        # å¿«é€Ÿç¡®å®šæ—¥æœŸèŒƒå›´
        if start_date is None or end_date is None:
            auto_start, auto_end = self._get_date_range_fast(coin_ids)
            actual_start = (
                datetime.strptime(start_date, "%Y-%m-%d").date()
                if start_date
                else auto_start
            )
            actual_end = (
                datetime.strptime(end_date, "%Y-%m-%d").date() if end_date else auto_end
            )
        else:
            actual_start = datetime.strptime(start_date, "%Y-%m-%d").date()
            actual_end = datetime.strptime(end_date, "%Y-%m-%d").date()

        self.logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {actual_start} åˆ° {actual_end}")

        # ç”Ÿæˆéœ€è¦å¤„ç†çš„æ—¥æœŸåˆ—è¡¨
        date_list = [
            actual_start + timedelta(days=x)
            for x in range((actual_end - actual_start).days + 1)
        ]

        total_days = len(date_list)
        self.logger.info(f"å°†å¤„ç† {total_days} å¤©çš„æ•°æ®")

        # æ™ºèƒ½è®¾ç½®çº¿ç¨‹æ•°
        if max_workers is None:
            # å¯¹äºé«˜æ€§èƒ½ç³»ç»Ÿï¼ŒI/Oå¯†é›†å‹ä»»åŠ¡å¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹
            cpu_count = multiprocessing.cpu_count()
            max_workers = min(cpu_count * 3, 64)  # æœ€å¤š64ä¸ªçº¿ç¨‹

        self.logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")

        # é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        self.logger.info("âš¡ é¢„åŠ è½½æ‰€æœ‰å¸ç§æ•°æ®åˆ°å†…å­˜...")
        all_coin_data = {}

        # ä½¿ç”¨çº¿ç¨‹æ± é¢„åŠ è½½æ•°æ®
        with ThreadPoolExecutor(max_workers=min(max_workers, 32)) as preload_executor:
            future_to_coin = {
                preload_executor.submit(
                    self._load_coin_data_optimized, coin_id
                ): coin_id
                for coin_id in coin_ids
            }

            for future in tqdm(
                as_completed(future_to_coin), total=len(coin_ids), desc="é¢„åŠ è½½æ•°æ®"
            ):
                coin_id = future_to_coin[future]
                all_coin_data[coin_id] = future.result()

        # å°†æ—¥æœŸåˆ†æ‰¹å¤„ç†
        date_batches = [
            date_list[i : i + self.batch_size]
            for i in range(0, len(date_list), self.batch_size)
        ]

        self.logger.info(
            f"åˆ†ä¸º {len(date_batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {self.batch_size} å¤©"
        )

        successful_days = 0

        # æ‰¹é‡å¹¶è¡Œå¤„ç†
        with tqdm(total=total_days, desc="ğŸ”¥ è¶…é«˜é€Ÿé‡å»ºæ¯æ—¥æ•°æ®") as pbar:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_batch = {
                    executor.submit(
                        self._process_date_batch, batch, all_coin_data
                    ): batch
                    for batch in date_batches
                }

                for future in as_completed(future_to_batch):
                    batch_success = future.result()
                    successful_days += batch_success
                    pbar.update(batch_success)

        self.logger.info("ğŸ‰ è¶…é«˜æ€§èƒ½é‡å»ºå®Œæˆ")
        self.logger.info(f"æ€»å¤„ç†å¤©æ•°: {total_days}")
        self.logger.info(f"æˆåŠŸç”Ÿæˆæ–‡ä»¶: {successful_days}")
        self.logger.info(f"æˆåŠŸç‡: {successful_days/total_days*100:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è¶…é«˜æ€§èƒ½æ¯æ—¥æ•°æ®æ±‡æ€»è„šæœ¬")

    parser.add_argument("--start-date", help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--coins-dir", default="data/coins", help="å¸ç§æ•°æ®ç›®å½•")
    parser.add_argument(
        "--output-dir", default="data/daily/daily_files", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument("--batch-size", type=int, default=100, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--max-workers", type=int, help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)

    try:
        # åˆ›å»ºæ±‡æ€»å™¨
        aggregator = UltraFastDailyAggregator(
            coins_dir=args.coins_dir,
            output_dir=args.output_dir,
            batch_size=args.batch_size,
        )

        # æ‰§è¡Œè¶…é«˜æ€§èƒ½é‡å»º
        aggregator.ultra_fast_rebuild(
            start_date=args.start_date,
            end_date=args.end_date,
            max_workers=args.max_workers,
        )

        logger.info("âœ… è¶…é«˜æ€§èƒ½æ¯æ—¥æ•°æ®æ±‡æ€»å®Œæˆ")

    except KeyboardInterrupt:
        logger.info("âŒ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ğŸ’¥ æ±‡æ€»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
