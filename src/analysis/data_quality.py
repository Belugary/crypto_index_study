"""
æ•°æ®è´¨é‡æ£€æŸ¥æ ¸å¿ƒæ¨¡å—

æä¾›æ•°æ®è´¨é‡åˆ†æã€éªŒè¯å’Œä¿®å¤åŠŸèƒ½çš„æ ¸å¿ƒå®ç°ã€‚
scripts/data_quality_checker.py æ˜¯æ­¤æ¨¡å—çš„ç”¨æˆ·æ¥å£å°è£…ã€‚
"""

import logging
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pandas as pd

from ..utils.path_utils import find_project_root

logger = logging.getLogger(__name__)


class DataQualityAnalyzer:
    """æ•°æ®è´¨é‡åˆ†æå™¨æ ¸å¿ƒç±»"""

    def __init__(self, data_dir: str = "data/coins", use_database: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®è´¨é‡åˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆæ¨èå¼€å¯ä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰
        """
        self.project_root = self._find_project_root()
        # è§£ææ•°æ®ç›®å½•è·¯å¾„
        if Path(data_dir).is_absolute():
            self.data_dir = Path(data_dir)
        else:
            self.data_dir = self.project_root / data_dir
        self.min_rows = 100
        self.max_days_old = 2
        self.min_data_span_days = 30
        self.use_database = use_database
        
        # ğŸš€ åˆå§‹åŒ–æ•°æ®åº“æ”¯æŒçš„æ•°æ®èšåˆå™¨
        if use_database:
            try:
                from ..downloaders.daily_aggregator import DailyDataAggregator
                self.daily_aggregator = DailyDataAggregator(
                    data_dir=str(self.data_dir),
                    output_dir=str(self.project_root / "data" / "daily"),
                    use_database=True
                )
            except ImportError:
                logger.warning("æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ–‡ä»¶æ¨¡å¼")
                self.use_database = False

    @staticmethod
    def _find_project_root() -> Path:
        """æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½• - ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„å·¥å…·"""
        return find_project_root()

    def _is_data_recent(self, data_span_days: int, days_since_latest: int) -> bool:
        """åˆ¤æ–­æ•°æ®æ˜¯å¦"æœ€æ–°"

        æ–°å¸ç§ç»™äºˆæ›´å®½æ¾çš„æ ‡å‡†ï¼Œè€å¸ç§ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†
        """
        if data_span_days < self.min_data_span_days:
            return days_since_latest <= 7  # æ–°å¸ç§
        else:
            return days_since_latest <= self.max_days_old  # è€å¸ç§

    def check_timestamp_intervals(
        self, df: pd.DataFrame, time_column: str
    ) -> Tuple[bool, str]:
        """æ£€æŸ¥æ—¶é—´æˆ³é—´éš”æ˜¯å¦åˆç†"""
        try:
            if time_column == "timestamp":
                dates = pd.to_datetime(df["timestamp"], unit="ms").dt.date
            else:
                dates = pd.to_datetime(df[time_column]).dt.date

            unique_dates = sorted(set(dates))
            if len(unique_dates) < 2:
                return True, "æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•æ£€æŸ¥é—´éš”"

            # æ£€æŸ¥è¶…è¿‡7å¤©çš„ç¼ºå¤±
            large_gaps = []
            for i in range(1, len(unique_dates)):
                gap_days = (unique_dates[i] - unique_dates[i - 1]).days
                if gap_days > 7:
                    large_gaps.append(
                        f"{unique_dates[i-1]} -> {unique_dates[i]} ({gap_days}å¤©)"
                    )

            if large_gaps:
                gap_info = "; ".join(large_gaps[:3])
                if len(large_gaps) > 3:
                    gap_info += f" ç­‰{len(large_gaps)}ä¸ªç¼ºå¤±"
                return False, f"å‘ç°å¤§æ—¶é—´ç¼ºå¤±: {gap_info}"

            return True, "æ—¶é—´é—´éš”æ­£å¸¸"

        except Exception as e:
            return True, f"æ—¶é—´é—´éš”æ£€æŸ¥å¤±è´¥: {str(e)}"

    def analyze_file_quality(self, filepath: str) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
        try:
            df = pd.read_csv(filepath)
            row_count = len(df)

            # æ£€æŸ¥æ—¶é—´åˆ—
            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"]).dt.date
                latest_date = df["date"].max()
                earliest_date = df["date"].min()
                interval_ok, interval_msg = self.check_timestamp_intervals(df, "date")
            elif "timestamp" in df.columns:
                df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
                latest_date = df["datetime"].dt.date.max()
                earliest_date = df["datetime"].dt.date.min()
                interval_ok, interval_msg = self.check_timestamp_intervals(
                    df, "timestamp"
                )
            else:
                return {
                    "rows": row_count,
                    "latest_date": None,
                    "earliest_date": None,
                    "data_span_days": 0,
                    "days_since_latest": 999,
                    "is_recent": False,
                    "has_enough_data": row_count >= self.min_rows,
                    "interval_ok": False,
                    "interval_msg": "æ— æ—¶é—´åˆ—",
                }

            data_span_days = (latest_date - earliest_date).days
            days_since_latest = (datetime.now().date() - latest_date).days

            return {
                "rows": row_count,
                "latest_date": latest_date,
                "earliest_date": earliest_date,
                "data_span_days": data_span_days,
                "days_since_latest": days_since_latest,
                "is_recent": self._is_data_recent(data_span_days, days_since_latest),
                "has_enough_data": row_count >= self.min_rows,
                "interval_ok": interval_ok,
                "interval_msg": interval_msg,
            }

        except Exception as e:
            return {
                "error": str(e),
                "rows": 0,
                "is_recent": False,
                "has_enough_data": False,
                "interval_ok": False,
                "interval_msg": f"æ£€æŸ¥å¤±è´¥: {str(e)}",
            }

    def scan_all_files(self) -> Tuple[List[Tuple], List[Tuple]]:
        """æ‰«ææ‰€æœ‰æ–‡ä»¶å¹¶åˆ†ç±»

        Returns:
            tuple: (good_files, problematic_files)
        """
        if not os.path.exists(self.data_dir):
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")

        files = [f for f in os.listdir(self.data_dir) if f.endswith(".csv")]
        good_files = []
        problematic_files = []

        for filename in files:
            filepath = os.path.join(self.data_dir, filename)
            coin_name = filename[:-4]
            quality = self.analyze_file_quality(filepath)

            if "error" in quality:
                problematic_files.append((coin_name, quality, "READ_ERROR"))
            elif not quality["has_enough_data"]:
                problematic_files.append((coin_name, quality, "INSUFFICIENT_DATA"))
            elif not quality["interval_ok"]:
                problematic_files.append((coin_name, quality, "INTERVAL_ISSUE"))
            elif not quality["is_recent"]:
                problematic_files.append((coin_name, quality, "OUTDATED_DATA"))
            else:
                good_files.append((coin_name, quality))

        return good_files, problematic_files


class DataQualityRepairer:
    """æ•°æ®è´¨é‡ä¿®å¤å™¨"""

    def __init__(self, analyzer: DataQualityAnalyzer):
        self.analyzer = analyzer
        self._updater = None

    def _get_updater(self):
        """å»¶è¿Ÿåˆå§‹åŒ–updater"""
        if self._updater is None:
            from src.updaters.price_updater import PriceDataUpdater

            self._updater = PriceDataUpdater()
        return self._updater

    def repair_files(
        self, problematic_files: List[Tuple], dry_run: bool = True
    ) -> List[Dict]:
        """ä¿®å¤æœ‰é—®é¢˜çš„æ–‡ä»¶

        Args:
            problematic_files: é—®é¢˜æ–‡ä»¶åˆ—è¡¨
            dry_run: æ˜¯å¦ä»…æ¨¡æ‹Ÿè¿è¡Œ

        Returns:
            ä¿®å¤ç»“æœåˆ—è¡¨
        """
        results = []

        for coin_name, quality, issue_type in problematic_files:
            result = {
                "coin_name": coin_name,
                "issue_type": issue_type,
                "success": False,
                "message": "",
            }

            if dry_run:
                result["message"] = "DRY RUN: å°†é‡æ–°ä¸‹è½½å®Œæ•´å†å²æ•°æ®"
                result["success"] = True
            else:
                try:
                    success, api_called = self._get_updater().download_coin_data(
                        coin_name
                    )
                    if success:
                        # é‡æ–°æ£€æŸ¥è´¨é‡
                        filepath = os.path.join(
                            self.analyzer.data_dir, f"{coin_name}.csv"
                        )
                        new_quality = self.analyzer.analyze_file_quality(filepath)
                        result["success"] = True
                        result["message"] = (
                            f"ä¿®å¤æˆåŠŸ: {new_quality['rows']}è¡Œ, æœ€æ–°:{new_quality['latest_date']}"
                        )
                    else:
                        result["message"] = "é‡æ–°ä¸‹è½½å¤±è´¥"
                except Exception as e:
                    result["message"] = f"ä¿®å¤é”™è¯¯: {e}"

            results.append(result)

        return results
