"""
é‡æ„åçš„æ•°æ®è´¨é‡æ£€æŸ¥æ¨¡å—

å°†åŸæœ‰çš„ DataQualityAnalyzer æ‹†åˆ†ä¸ºï¼š
1. DataQualityChecker - çº¯æ£€æŸ¥åŠŸèƒ½
2. DataQualityConfig - é…ç½®ç®¡ç†
3. å·¥å…·å‡½æ•° - ç‹¬ç«‹çš„æ£€æŸ¥é€»è¾‘
"""

import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pandas as pd

from .data_quality_config import DataQualityConfig, ResolvedPaths

logger = logging.getLogger(__name__)


class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨ï¼ˆçº¯æ£€æŸ¥ï¼Œä¸åšä¿®å¤ï¼‰"""

    def __init__(self, config: Optional[DataQualityConfig] = None):
        """
        åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å™¨
        
        Args:
            config: æ•°æ®è´¨é‡é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†é…ç½®
        """
        self.config = config or DataQualityConfig()
        self.paths = self.config.resolve_paths()
        
        # ğŸš€ åˆå§‹åŒ–æ•°æ®åº“æ”¯æŒçš„æ•°æ®èšåˆå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.daily_aggregator = None
        if self.config.use_database:
            try:
                from ..downloaders.daily_aggregator import DailyDataAggregator
                self.daily_aggregator = DailyDataAggregator(
                    data_dir=str(self.paths.data_dir),
                    output_dir=str(self.paths.daily_dir),
                    use_database=True
                )
            except ImportError:
                logger.warning("æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ–‡ä»¶æ¨¡å¼")

    def check_file_quality(self, filepath: Path) -> Dict:
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
        return analyze_file_quality(
            filepath, 
            self.config.min_rows,
            self.config.max_days_old,
            self.config.min_data_span_days
        )

    def scan_all_files(self) -> Tuple[List[Tuple], List[Tuple]]:
        """æ‰«ææ‰€æœ‰æ–‡ä»¶å¹¶åˆ†ç±»

        Returns:
            tuple: (good_files, problematic_files)
        """
        if not self.paths.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.paths.data_dir}")

        files = [f for f in self.paths.data_dir.iterdir() if f.suffix == ".csv"]
        good_files = []
        problematic_files = []

        for filepath in files:
            coin_name = filepath.stem
            quality = self.check_file_quality(filepath)

            if "error" in quality:
                problematic_files.append((coin_name, quality, "READ_ERROR"))
            elif not quality["has_enough_data"]:
                problematic_files.append((coin_name, quality, "INSUFFICIENT_DATA"))
            elif not quality["interval_ok"]:
                problematic_files.append((coin_name, quality, "INTERVAL_ISSUE"))
            elif not quality["is_recent"]:
                problematic_files.append((coin_name, quality, "OUTDATED_DATA"))
            else:
                good_files.append((coin_name, quality))

        return good_files, problematic_files

    def get_quality_summary(self) -> Dict:
        """è·å–æ•°æ®è´¨é‡æ‘˜è¦"""
        good_files, problematic_files = self.scan_all_files()
        
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
        issue_counts = {}
        for _, _, issue_type in problematic_files:
            issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
        
        return {
            "total_files": len(good_files) + len(problematic_files),
            "good_files": len(good_files),
            "problematic_files": len(problematic_files),
            "issue_breakdown": issue_counts,
            "good_files_list": [name for name, _ in good_files],
            "problematic_files_list": [(name, issue) for name, _, issue in problematic_files]
        }


# å·¥å…·å‡½æ•° - ç‹¬ç«‹çš„æ£€æŸ¥é€»è¾‘

def is_data_recent(data_span_days: int, days_since_latest: int, 
                  min_data_span_days: int, max_days_old: int) -> bool:
    """åˆ¤æ–­æ•°æ®æ˜¯å¦"æœ€æ–°"

    æ–°å¸ç§ç»™äºˆæ›´å®½æ¾çš„æ ‡å‡†ï¼Œè€å¸ç§ä½¿ç”¨ä¸¥æ ¼æ ‡å‡†
    """
    if data_span_days < min_data_span_days:
        return days_since_latest <= 7  # æ–°å¸ç§
    else:
        return days_since_latest <= max_days_old  # è€å¸ç§


def check_timestamp_intervals(df: pd.DataFrame, time_column: str) -> Tuple[bool, str]:
    """æ£€æŸ¥æ—¶é—´æˆ³é—´éš”æ˜¯å¦åˆç†"""
    try:
        if time_column == "timestamp":
            dates = pd.to_datetime(df["timestamp"], unit="ms").dt.date
        else:
            dates = pd.to_datetime(df[time_column]).dt.date

        unique_dates = sorted(set(dates))
        if len(unique_dates) < 2:
            return True, "æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•æ£€æŸ¥é—´éš”"

        # æ£€æŸ¥è¶…è¿‡7å¤©çš„ç¼ºå¤±
        large_gaps = []
        for i in range(1, len(unique_dates)):
            gap_days = (unique_dates[i] - unique_dates[i - 1]).days
            if gap_days > 7:
                large_gaps.append(
                    f"{unique_dates[i-1]} -> {unique_dates[i]} ({gap_days}å¤©)"
                )

        if large_gaps:
            gap_info = "; ".join(large_gaps[:3])
            if len(large_gaps) > 3:
                gap_info += f" ç­‰{len(large_gaps)}ä¸ªç¼ºå¤±"
            return False, f"å‘ç°å¤§æ—¶é—´ç¼ºå¤±: {gap_info}"

        return True, "æ—¶é—´é—´éš”æ­£å¸¸"

    except Exception as e:
        return True, f"æ—¶é—´é—´éš”æ£€æŸ¥å¤±è´¥: {str(e)}"


def analyze_file_quality(filepath: Path, min_rows: int, max_days_old: int, 
                        min_data_span_days: int) -> Dict:
    """åˆ†æå•ä¸ªæ–‡ä»¶çš„æ•°æ®è´¨é‡"""
    try:
        df = pd.read_csv(filepath)
        row_count = len(df)

        # æ£€æŸ¥æ—¶é—´åˆ—
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"]).dt.date
            latest_date = df["date"].max()
            earliest_date = df["date"].min()
            interval_ok, interval_msg = check_timestamp_intervals(df, "date")
        elif "timestamp" in df.columns:
            df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
            latest_date = df["datetime"].dt.date.max()
            earliest_date = df["datetime"].dt.date.min()
            interval_ok, interval_msg = check_timestamp_intervals(df, "timestamp")
        else:
            return {
                "rows": row_count,
                "latest_date": None,
                "earliest_date": None,
                "data_span_days": 0,
                "days_since_latest": 999,
                "is_recent": False,
                "has_enough_data": row_count >= min_rows,
                "interval_ok": False,
                "interval_msg": "æ— æ—¶é—´åˆ—",
            }

        data_span_days = (latest_date - earliest_date).days
        days_since_latest = (datetime.now().date() - latest_date).days

        return {
            "rows": row_count,
            "latest_date": latest_date,
            "earliest_date": earliest_date,
            "data_span_days": data_span_days,
            "days_since_latest": days_since_latest,
            "is_recent": is_data_recent(data_span_days, days_since_latest, 
                                      min_data_span_days, max_days_old),
            "has_enough_data": row_count >= min_rows,
            "interval_ok": interval_ok,
            "interval_msg": interval_msg,
        }

    except Exception as e:
        return {
            "error": str(e),
            "rows": 0,
            "is_recent": False,
            "has_enough_data": False,
            "interval_ok": False,
            "interval_msg": f"æ£€æŸ¥å¤±è´¥: {str(e)}",
        }


# ä¾¿æ·å‡½æ•°

def create_data_quality_checker(data_dir: str = "data/coins", 
                               use_database: bool = True) -> DataQualityChecker:
    """åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥å™¨çš„ä¾¿æ·å‡½æ•°"""
    config = DataQualityConfig(data_dir=data_dir, use_database=use_database)
    return DataQualityChecker(config)
