#!/usr/bin/env python3
"""
æ¯æ—¥æ•°æ®èšåˆå™¨

åŸºäºå·²ä¸‹è½½çš„å†å²æ•°æ®ï¼Œæ„å»ºæŒ‰æ—¥æœŸç»„ç»‡çš„æ•°æ®é›†åˆï¼Œ
ç”¨äºåˆ†ææ¯æ—¥å¸‚åœºæ„æˆå’Œæ„å»ºå†å²æŒ‡æ•°ã€‚

âš ï¸ æ•°æ®å­—æ®µè¯´æ˜:
- price: å½“æ—¥ä»·æ ¼ (USD)
- volume: å½“æ—¥24å°æ—¶äº¤æ˜“é‡ (USD)
- market_cap: å½“æ—¥æµé€šå¸‚å€¼ (USD)
  é‡è¦: è¿™æ˜¯æµé€šå¸‚å€¼ (Circulating Market Cap)ï¼Œè®¡ç®—å…¬å¼ä¸ºä»·æ ¼Ã—æµé€šä¾›åº”é‡
  è¿™æ˜¯é‡‘èæŒ‡æ•°ç¼–åˆ¶çš„æ ‡å‡†åšæ³•ï¼Œæ›´å‡†ç¡®åæ˜ å¯äº¤æ˜“ä»·å€¼
"""

import glob
import logging
import multiprocessing
import os
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ•°æ®åº“æ”¯æŒ - å¯é€‰å¯¼å…¥ï¼Œä¿æŒå‘åå…¼å®¹
try:
    from src.utils.database_utils import DatabaseManager
    DATABASE_AVAILABLE = True
    logger.info("æ•°æ®åº“æ”¯æŒå·²å¯ç”¨")
except ImportError:
    DATABASE_AVAILABLE = False
    logger.warning("æ•°æ®åº“æ”¯æŒä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CSVæ¨¡å¼")

# å¯¼å…¥è·¯å¾„å·¥å…·
from ..utils.path_utils import find_project_root, resolve_data_path


class DailyDataAggregator:
    """æ¯æ—¥æ•°æ®èšåˆå™¨

    åŠŸèƒ½ï¼š
    1. æ‰«ææ‰€æœ‰å·²ä¸‹è½½çš„å¸ç§æ•°æ®
    2. æŒ‰æ—¥æœŸèšåˆæ•°æ®ï¼Œæ„å»ºæ¯æ—¥å¸‚åœºå¿«ç…§
    3. æä¾›æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„å¸‚åœºæ•°æ®åŠŸèƒ½
    4. åˆ†ææ•°æ®è¦†ç›–èŒƒå›´å’Œè´¨é‡
    """

    @staticmethod
    def read_daily_snapshot(date_str: str, daily_dir: str = "data/daily/daily_files", result_include_all: bool = False) -> pd.DataFrame:
        """
        ğŸ“ é™æ€æ–¹æ³•ï¼šè¯»å–å·²èšåˆçš„æ¯æ—¥å¸‚åœºå¿«ç…§ CSV
        
        ğŸ¯ ä¼˜åŠ¿: ä¸éœ€è¦å®ä¾‹åŒ– DailyDataAggregatorï¼Œç›´æ¥è¯»å–æ–‡ä»¶
        ğŸ“Œ ç”¨é€”: é€‚åˆå¿«é€Ÿè·å–å†å²æ•°æ®ï¼Œé¿å…é‡å¤åŠ è½½æ‰€æœ‰å¸ç§æ•°æ®
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            daily_dir: æ¯æ—¥å¿«ç…§æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ 'data/daily/daily_files'
            result_include_all: æ˜¯å¦åŒ…å«æ‰€æœ‰å¸ç§
                              - True: è¿”å›å…¨éƒ¨å¸ç§ (åŒ…æ‹¬ç¨³å®šå¸ã€åŒ…è£…å¸)
                              - False: åªè¿”å›åŸç”Ÿå¸ç§ (æ’é™¤ç¨³å®šå¸ã€åŒ…è£…å¸)

        Returns:
            æŒ‡å®šæ—¥æœŸçš„å¸‚åœºå¿«ç…§ DataFrameï¼Œæ ¹æ®result_include_allå‚æ•°è¿‡æ»¤
            è‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©º DataFrame
        """
        # è§£ææ—¥æœŸå¹¶æ„å»ºåˆ†å±‚è·¯å¾„
        try:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d").date()
            year = date_obj.strftime("%Y")
            month = date_obj.strftime("%m")
            file_path = Path(daily_dir) / year / month / f"{date_str}.csv"
        except ValueError:
            logger.error(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {date_str}ï¼Œåº”ä¸º YYYY-MM-DD")
            return pd.DataFrame()
        
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"]).dt.date
        
        # å¦‚æœéœ€è¦ï¼Œè¿‡æ»¤å‡ºåŸç”Ÿå¸ç§
        if not result_include_all and not df.empty:
            from src.classification.unified_classifier import UnifiedClassifier
            
            # æ¨æ–­æ•°æ®æ ¹ç›®å½•
            data_dir = Path(daily_dir).parent.parent
            classifier = UnifiedClassifier(data_dir=str(data_dir))
            coin_ids = df['coin_id'].unique().tolist()
            
            native_coin_ids = classifier.filter_coins(
                coin_ids=coin_ids,
                exclude_stablecoins=True,
                exclude_wrapped_coins=True,
                use_cache=True
            )
            
            df = df[df['coin_id'].isin(native_coin_ids)].copy()
            
        return df

    def __init__(self, data_dir: Optional[str] = None, output_dir: Optional[str] = None, result_include_all: bool = False, 
                 use_database: bool = False, db_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¯æ—¥æ•°æ®èšåˆå™¨
        
        ğŸ“ è·¯å¾„ç®¡ç†: è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•ï¼Œé¿å…åœ¨å­ç›®å½•åˆ›å»ºé”™è¯¯çš„dataæ–‡ä»¶å¤¹
        ğŸš€ æ•°æ®åº“æ”¯æŒ: å¯é€‰æ‹©ä½¿ç”¨é«˜æ€§èƒ½æ•°æ®åº“æ¨¡å¼ï¼ˆæ¯”CSVå¿«100-1000å€ï¼‰
        âš ï¸ åºŸå¼ƒå‚æ•°: result_include_all åœ¨æ„é€ å‡½æ•°ä¸­å·²åºŸå¼ƒï¼Œè¯·åœ¨ get_daily_data() æ–¹æ³•ä¸­ä½¿ç”¨

        Args:
            data_dir: åŸå§‹CSVæ•°æ®ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„data/coinsï¼‰
            output_dir: èšåˆåæ•°æ®è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„data/dailyï¼‰
            result_include_all: [å·²åºŸå¼ƒ] æ­¤å‚æ•°åœ¨æ„é€ å‡½æ•°ä¸­æ— æ•ˆï¼Œè¯·åœ¨è°ƒç”¨ get_daily_data() æ—¶æŒ‡å®š
                              ä¿ç•™æ­¤å‚æ•°ä»…ä¸ºå‘åå…¼å®¹ï¼Œå»ºè®®ä½¿ç”¨æ–¹æ³•çº§åˆ«çš„å‚æ•°æ§åˆ¶
            use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆéœ€è¦å·²å®Œæˆæ•°æ®åº“è¿ç§»ï¼‰
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º data/crypto_market.dbï¼‰
        """
        # ä½¿ç”¨æ–°çš„è·¯å¾„å·¥å…·
        self.project_root = find_project_root()
        
        # é»˜è®¤è·¯å¾„ï¼šå§‹ç»ˆåŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼Œé¿å…åœ¨å­ç›®å½•åˆ›å»ºæ–‡ä»¶å¤¹
        if data_dir is None:
            data_dir = "data/coins"
        if output_dir is None:
            output_dir = "data/daily"
        
        # è·¯å¾„è§£æï¼šä½¿ç”¨æ–°çš„è·¯å¾„å·¥å…·
        self.data_dir = resolve_data_path(data_dir, self.project_root)
        self.output_dir = resolve_data_path(output_dir, self.project_root)
        
        # åªåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆé™¤éæ˜¾å¼æŒ‡å®šç»å¯¹è·¯å¾„ï¼‰
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ result_include_all è®¾ç½®
        self.result_include_all = result_include_all
        
        # è®¾ç½®æ—¥å¿—ç›®å½•ï¼šå§‹ç»ˆåœ¨é¡¹ç›®æ ¹ç›®å½•
        self.log_folder = self.project_root / "logs"
        self.log_folder.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–logger
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # åˆ›å»ºå­ç›®å½•ç”¨äºå­˜å‚¨ä¸åŒç±»å‹çš„æ•°æ®
        self.daily_files_dir = self.output_dir / "daily_files"
        self.daily_files_dir.mkdir(parents=True, exist_ok=True)

        # ç¼“å­˜
        self.daily_cache: Dict[str, pd.DataFrame] = {}
        self.coin_data: Dict[str, pd.DataFrame] = {}
        self.loaded_coins: List[str] = []
        
        # ğŸš€ æ•°æ®åº“æ”¯æŒé…ç½®
        self.use_database = use_database and DATABASE_AVAILABLE
        self.db_manager = None
        
        if self.use_database:
            try:
                # é»˜è®¤æ•°æ®åº“è·¯å¾„
                if db_path is None:
                    db_path = str(self.project_root / "data" / "crypto_market.db")
                
                self.db_manager = DatabaseManager(db_path)
                self.logger.info(f"æ•°æ®åº“æ¨¡å¼å·²å¯ç”¨: {db_path}")
                
                # éªŒè¯æ•°æ®åº“å¥åº·çŠ¶æ€
                stats = self.db_manager.get_database_stats()
                self.logger.info(f"æ•°æ®åº“ç»Ÿè®¡: {stats.get('total_coins', 0):,} å¸ç§, {stats.get('total_records', 0):,} æ¡è®°å½•")
                
            except Exception as e:
                self.logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CSVæ¨¡å¼: {e}")
                self.use_database = False
                self.db_manager = None
        
        if not self.use_database:
            self.logger.info("ä½¿ç”¨CSVæ–‡ä»¶æ¨¡å¼")
        
        logger.info(
            f"æ¯æ—¥æ•°æ®èšåˆå™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {'æ•°æ®åº“' if self.use_database else 'CSV'}, æ•°æ®æº: '{self.data_dir}', è¾“å‡ºåˆ°: '{self.output_dir}'"
        )

        # æ—¥æœŸèŒƒå›´ä¿¡æ¯
        self.min_date: Optional[datetime] = None
        self.max_date: Optional[datetime] = None

        logger.info(f"åˆå§‹åŒ–æ¯æ—¥æ•°æ®èšåˆå™¨")
        logger.info(f"æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"æ¯æ—¥æ–‡ä»¶ç›®å½•: {self.daily_files_dir}")

    def load_coin_data(self) -> None:
        """åŠ è½½æ‰€æœ‰å¸ç§çš„CSVæ•°æ®åˆ°å†…å­˜"""
        logger.info("å¼€å§‹ä»CSVæ–‡ä»¶åŠ è½½æ‰€æœ‰å¸ç§æ•°æ®åˆ°å†…å­˜...")
        csv_files = list(self.data_dir.glob("*.csv"))
        if not csv_files:
            logger.warning(f"æ•°æ®ç›®å½• '{self.data_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶ã€‚")
            return

        for file_path in csv_files:
            coin_id = file_path.stem
            try:
                df = pd.read_csv(file_path)
                if df.empty:
                    logger.warning(f"è·³è¿‡ç©ºæ–‡ä»¶: {file_path}")
                    continue

                # è½¬æ¢æ—¶é—´æˆ³å¹¶åˆ›å»º 'date' åˆ—
                df["timestamp"] = pd.to_numeric(df["timestamp"], errors="coerce")
                df.dropna(subset=["timestamp"], inplace=True)
                df["date"] = pd.to_datetime(df["timestamp"], unit="ms").dt.strftime(
                    "%Y-%m-%d"
                )
                df["coin_id"] = coin_id
                self.coin_data[coin_id] = df
                self.loaded_coins.append(coin_id)
                logger.debug(f"æˆåŠŸåŠ è½½ {coin_id} ({len(df)}æ¡è®°å½•)")
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(self.loaded_coins)} ä¸ªå¸ç§çš„æ•°æ®ã€‚")

    def _calculate_date_range(self) -> None:
        """è®¡ç®—æ‰€æœ‰æ•°æ®çš„æ—¥æœŸèŒƒå›´"""
        if not self.coin_data:
            return

        all_dates = []
        for df in self.coin_data.values():
            all_dates.extend(df["date"].tolist())

        if all_dates:
            # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸º datetime å¯¹è±¡ä»¥æ”¯æŒæ—¥æœŸè¿ç®—
            from datetime import datetime

            date_objects = [datetime.strptime(d, "%Y-%m-%d") for d in all_dates]
            self.min_date = min(date_objects)
            self.max_date = max(date_objects)

            logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {self.min_date} åˆ° {self.max_date}")

            # è®¡ç®—æ€»å¤©æ•°
            if self.min_date and self.max_date:
                total_days = (self.max_date - self.min_date).days + 1
                logger.info(f"æ€»å…± {total_days} å¤©çš„æ•°æ®")
    
    def get_available_dates_from_database(self) -> List[str]:
        """
        ğŸš€ ä»æ•°æ®åº“è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸï¼ˆæ•°æ®åº“æ¨¡å¼ä¸“ç”¨ï¼‰
        
        Returns:
            å¯ç”¨æ—¥æœŸåˆ—è¡¨ï¼Œå¦‚æœæ•°æ®åº“ä¸å¯ç”¨åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        if not self.use_database or not self.db_manager:
            logger.warning("æ•°æ®åº“æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·ä½¿ç”¨ CSV æ¨¡å¼æˆ–å¯ç”¨æ•°æ®åº“")
            return []
        
        try:
            dates = self.db_manager.get_available_dates()
            logger.info(f"æ•°æ®åº“ä¸­æœ‰ {len(dates)} ä¸ªå¯ç”¨æ—¥æœŸ")
            return dates
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–å¯ç”¨æ—¥æœŸå¤±è´¥: {e}")
            return []
    
    def get_database_market_summary(self, target_date: str, limit: int = 10) -> pd.DataFrame:
        """
        ğŸš€ è·å–æ•°æ®åº“ä¸­æŒ‡å®šæ—¥æœŸçš„å¸‚å€¼å‰Nå¸ç§æ‘˜è¦ï¼ˆæ•°æ®åº“æ¨¡å¼ä¸“ç”¨ï¼‰
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
            limit: è¿”å›å¸ç§æ•°é‡ï¼Œé»˜è®¤å‰10
            
        Returns:
            å‰Nå¤§å¸ç§DataFrameï¼Œå¦‚æœæ•°æ®åº“ä¸å¯ç”¨åˆ™è¿”å›ç©ºDataFrame
        """
        if not self.use_database or not self.db_manager:
            logger.warning("æ•°æ®åº“æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·ä½¿ç”¨ get_daily_data() æ–¹æ³•")
            return pd.DataFrame()
        
        try:
            df = self.db_manager.get_top_coins_by_market_cap(target_date, limit)
            logger.info(f"è·å– {target_date} å‰{limit}å¤§å¸ç§æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            logger.error(f"è·å–å¸‚åœºæ‘˜è¦å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_coin_price_history_from_database(self, coin_id: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ğŸš€ ä»æ•°æ®åº“è·å–å¸ç§ä»·æ ¼å†å²ï¼ˆæ•°æ®åº“æ¨¡å¼ä¸“ç”¨ï¼‰
        
        Args:
            coin_id: å¸ç§ID
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            ä»·æ ¼å†å²DataFrameï¼Œå¦‚æœæ•°æ®åº“ä¸å¯ç”¨åˆ™è¿”å›ç©ºDataFrame
        """
        if not self.use_database or not self.db_manager:
            logger.warning("æ•°æ®åº“æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·ä½¿ç”¨ CSV æ¨¡å¼")
            return pd.DataFrame()
        
        try:
            df = self.db_manager.get_price_history(coin_id, start_date, end_date)
            logger.info(f"è·å– {coin_id} ä»·æ ¼å†å² ({start_date} è‡³ {end_date}): {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            logger.error(f"è·å–ä»·æ ¼å†å²å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def switch_to_database_mode(self, db_path: Optional[str] = None) -> bool:
        """
        ğŸš€ è¿è¡Œæ—¶åˆ‡æ¢åˆ°æ•°æ®åº“æ¨¡å¼
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ°æ•°æ®åº“æ¨¡å¼
        """
        if not DATABASE_AVAILABLE:
            logger.error("æ•°æ®åº“æ”¯æŒä¸å¯ç”¨ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–")
            return False
        
        try:
            if db_path is None:
                db_path = str(self.project_root / "data" / "crypto_market.db")
            
            self.db_manager = DatabaseManager(db_path)
            self.use_database = True
            
            # éªŒè¯æ•°æ®åº“
            stats = self.db_manager.get_database_stats()
            logger.info(f"æˆåŠŸåˆ‡æ¢åˆ°æ•°æ®åº“æ¨¡å¼: {stats.get('total_coins', 0):,} å¸ç§, {stats.get('total_records', 0):,} æ¡è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"åˆ‡æ¢åˆ°æ•°æ®åº“æ¨¡å¼å¤±è´¥: {e}")
            self.use_database = False
            self.db_manager = None
            return False
    
    def switch_to_csv_mode(self) -> None:
        """
        ğŸ“ è¿è¡Œæ—¶åˆ‡æ¢åˆ°CSVæ¨¡å¼
        """
        self.use_database = False
        self.db_manager = None
        logger.info("åˆ‡æ¢åˆ°CSVæ–‡ä»¶æ¨¡å¼")

    def get_daily_data(
        self, target_date: Union[str, datetime, date], force_refresh: bool = False, result_include_all: bool = False,
        prefer_database: bool = True, skip_filter: bool = False
    ) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„èšåˆå¸‚åœºæ•°æ®

        ğŸš€ æ•°æ®åº“æ¨¡å¼: å¦‚æœå¯ç”¨æ•°æ®åº“ï¼Œä¼˜å…ˆä½¿ç”¨é«˜æ€§èƒ½æ•°æ®åº“æŸ¥è¯¢ï¼ˆæ¯”CSVå¿«100-1000å€ï¼‰
        â­ æ ¸å¿ƒå‚æ•°è¯´æ˜ï¼ˆæœ€è¿‘ä¿®å¤é‡ç‚¹ï¼‰:
        - force_refresh: æ§åˆ¶æ•°æ®æ¥æº (ç¼“å­˜ vs é‡æ–°è®¡ç®—)
        - result_include_all: æ§åˆ¶è¿‡æ»¤é€»è¾‘ (åŸç”Ÿå¸ vs å…¨éƒ¨å¸ç§)
        - prefer_database: å¦‚æœæ•°æ®åº“å¯ç”¨ï¼Œæ˜¯å¦ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢
        - skip_filter: è·³è¿‡åˆ†ç±»è¿‡æ»¤ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
        
        ğŸ“Š æ•°æ®è·å–ä¼˜å…ˆçº§:
        1. æ•°æ®åº“æŸ¥è¯¢ (prefer_database=True ä¸”æ•°æ®åº“å¯ç”¨)
        2. å†…å­˜ç¼“å­˜ (force_refresh=False ä¸”å­˜åœ¨)
        3. æ–‡ä»¶ç¼“å­˜ (force_refresh=False ä¸”æ–‡ä»¶å­˜åœ¨) 
        4. é‡æ–°è®¡ç®— (force_refresh=True æˆ–æ— ç¼“å­˜)
        
        ğŸ” è¿‡æ»¤é€»è¾‘åº”ç”¨:
        - skip_filter=True: è·³è¿‡æ‰€æœ‰è¿‡æ»¤ï¼Œè¿”å›åŸå§‹æ•°æ®ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
        - result_include_all=True: åŒ…å«ç¨³å®šå¸ã€åŒ…è£…å¸ç­‰æ‰€æœ‰å¸ç§
        - result_include_all=False: åªåŒ…å«åŸç”Ÿå¸ç§ï¼Œæ’é™¤ç¨³å®šå¸å’ŒåŒ…è£…å¸
        - âš ï¸ è¿‡æ»¤åœ¨æ‰€æœ‰æ•°æ®è·å–è·¯å¾„åç»Ÿä¸€åº”ç”¨

        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ”¯æŒå­—ç¬¦ä¸²ã€datetimeæˆ–dateç±»å‹
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼Œå¿½ç•¥æ‰€æœ‰ç¼“å­˜ (å†…å­˜+æ–‡ä»¶)
            result_include_all: ç»“æœæ˜¯å¦åŒ…å«æ‰€æœ‰å¸ç§ï¼ŒFalseæ—¶åªè¿”å›åŸç”Ÿå¸ç§
            prefer_database: ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            skip_filter: è·³è¿‡åˆ†ç±»è¿‡æ»¤ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰

        Returns:
            åŒ…å«æŒ‡å®šæ—¥æœŸå¸‚åœºæ•°æ®çš„DataFrameï¼Œæ ¹æ®result_include_allå‚æ•°è¿‡æ»¤
        """
        if isinstance(target_date, str):
            try:
                target_date_dt = datetime.strptime(target_date, "%Y-%m-%d")
            except ValueError:
                logger.error(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {target_date}ï¼Œåº”ä¸º YYYY-MM-DD")
                return pd.DataFrame()
        elif isinstance(target_date, datetime):
            target_date_dt = target_date
        elif isinstance(target_date, date):
            # å°† date å¯¹è±¡è½¬æ¢ä¸º datetime å¯¹è±¡ï¼Œæ—¶é—´è®¾ä¸ºåˆå¤œ
            target_date_dt = datetime.combine(target_date, datetime.min.time())
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ—¥æœŸç±»å‹: {type(target_date)}")
            return pd.DataFrame()

        target_date_str = target_date_dt.strftime("%Y-%m-%d")

        # ğŸš€ æ­¥éª¤0: æ•°æ®åº“æŸ¥è¯¢ (æœ€é«˜æ€§èƒ½ - å¦‚æœå¯ç”¨ä¸”ä¸å¼ºåˆ¶åˆ·æ–°)
        if self.use_database and self.db_manager and prefer_database and not force_refresh:
            try:
                logger.info(f"ä»æ•°æ®åº“æŸ¥è¯¢ {target_date_str} çš„æ•°æ®")
                import time
                start_time = time.time()
                
                df = self.db_manager.get_daily_market_data(target_date_str)
                
                query_time = (time.time() - start_time) * 1000
                logger.info(f"æ•°æ®åº“æŸ¥è¯¢å®Œæˆ: {len(df)} æ¡è®°å½•, è€—æ—¶ {query_time:.2f}ms")
                
                if not df.empty:
                    # æ ‡å‡†åŒ–åˆ—åï¼Œç¡®ä¿ä¸CSVæ ¼å¼ä¸€è‡´
                    if 'coin_id' not in df.columns and 'id' in df.columns:
                        df = df.rename(columns={'id': 'coin_id'})
                    
                    # ç¼“å­˜åˆ°å†…å­˜ï¼ˆå¯é€‰ï¼Œç”¨äºåç»­è®¿é—®ï¼‰
                    self.daily_cache[target_date_str] = df.copy()
                    
                    # åº”ç”¨è¿‡æ»¤å¹¶è¿”å›
                    if skip_filter:
                        return df
                    else:
                        return self._apply_result_filter(df, result_include_all)
                else:
                    logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ° {target_date_str} çš„æ•°æ®ï¼Œå°è¯•CSVæ¨¡å¼")
                    
            except Exception as e:
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CSVæ¨¡å¼: {e}")

        # ğŸ§  æ­¥éª¤1: æ£€æŸ¥å†…å­˜ç¼“å­˜ (æœ€å¿«)
        # âœ… ä¿®å¤å: ç¼“å­˜æ•°æ®ä¹Ÿä¼šåº”ç”¨ result_include_all è¿‡æ»¤
        if not force_refresh and target_date_str in self.daily_cache:
            logger.info(f"ä»å†…å­˜ç¼“å­˜åŠ è½½ {target_date_str} çš„æ•°æ®")
            cached_df = self.daily_cache[target_date_str]
            if skip_filter:
                return cached_df
            else:
                return self._apply_result_filter(cached_df, result_include_all)

        # ğŸ“ æ­¥éª¤2: æ£€æŸ¥æ–‡ä»¶ç¼“å­˜ (ä¸­ç­‰é€Ÿåº¦)
        # âœ… ä¿®å¤å: æ–‡ä»¶ç¼“å­˜æ•°æ®ä¹Ÿä¼šåº”ç”¨ result_include_all è¿‡æ»¤
        daily_file_path = self._get_daily_file_path(target_date_dt.date())

        if not force_refresh and daily_file_path.exists():
            logger.info(f"ä»ç¼“å­˜æ–‡ä»¶åŠ è½½ {target_date_str} çš„æ•°æ®: {daily_file_path}")
            try:
                df = pd.read_csv(daily_file_path)
                # ç¡®ä¿ 'date' åˆ—æ˜¯ datetime å¯¹è±¡ä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒ
                if "date" in df.columns:
                    df["date"] = pd.to_datetime(df["date"]).dt.date
                self.daily_cache[target_date_str] = df  # æ›´æ–°ç¼“å­˜
                if skip_filter:
                    return df
                else:
                    return self._apply_result_filter(df, result_include_all)
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶ {daily_file_path} å¤±è´¥ï¼Œå°†é‡æ–°è®¡ç®—: {e}")

        # ğŸ’¾ æ­¥éª¤3: é‡æ–°è®¡ç®—æ•°æ® (æœ€æ…¢ï¼Œä½†æ•°æ®æœ€æ–°)
        # å½“ force_refresh=True æˆ–æ— ç¼“å­˜æ—¶æ‰§è¡Œ
        if not self.coin_data:
            logger.info("å†…å­˜ä¸­æ— å¸ç§æ•°æ®ï¼Œå¼€å§‹åŠ è½½...")
            self.load_coin_data()

        logger.info(
            f"å¼€å§‹ä¸º {target_date_str} è®¡ç®—æ¯æ—¥æ•°æ® (å¼ºåˆ¶åˆ·æ–°: {force_refresh})"
        )
        daily_df = self._compute_daily_data(target_date_dt.date())

        # ä¿å­˜åˆ°æ–‡ä»¶å’Œç¼“å­˜
        if not daily_df.empty:
            daily_df.to_csv(daily_file_path, index=False)
            logger.info(f"å·²å°† {target_date_str} çš„æ•°æ®ä¿å­˜åˆ° {daily_file_path}")
            self.daily_cache[target_date_str] = daily_df

        # ğŸ” æ­¥éª¤4: åº”ç”¨è¿‡æ»¤é€»è¾‘ (ç»Ÿä¸€åœ¨æ­¤å¤„å¤„ç†)
        # âœ… ä¿®å¤é‡ç‚¹: ç¡®ä¿æ‰€æœ‰æ•°æ®è·å–è·¯å¾„éƒ½ä¼šåº”ç”¨æ­¤è¿‡æ»¤
        if skip_filter:
            return daily_df
        else:
            return self._apply_result_filter(daily_df, result_include_all)

    def _apply_result_filter(self, df: pd.DataFrame, result_include_all: bool) -> pd.DataFrame:
        """
        ğŸ” æ ¸å¿ƒè¿‡æ»¤æ–¹æ³•ï¼šæ ¹æ® result_include_all å‚æ•°ç»Ÿä¸€åº”ç”¨è¿‡æ»¤é€»è¾‘
        
        ğŸ“Œ ä¿®å¤è¯´æ˜: æ­¤æ–¹æ³•ç¡®ä¿æ‰€æœ‰æ•°æ®è·å–è·¯å¾„(å†…å­˜ç¼“å­˜/æ–‡ä»¶ç¼“å­˜/é‡æ–°è®¡ç®—)
                   éƒ½ä¼šç»Ÿä¸€åº”ç”¨ result_include_all è¿‡æ»¤ï¼Œè§£å†³ä¹‹å‰ç¼“å­˜å¿½ç•¥è¿‡æ»¤çš„Bug
        
        Args:
            df: å¾…è¿‡æ»¤çš„ DataFrame (åŒ…å«æ‰€æœ‰å¸ç§æ•°æ®)
            result_include_all: æ˜¯å¦åŒ…å«æ‰€æœ‰å¸ç§
                              - True: è¿”å›å…¨éƒ¨å¸ç§ (åŒ…æ‹¬ç¨³å®šå¸ã€åŒ…è£…å¸)
                              - False: åªè¿”å›åŸç”Ÿå¸ç§ (æ’é™¤ç¨³å®šå¸ã€åŒ…è£…å¸)
            
        Returns:
            æ ¹æ®å‚æ•°è¿‡æ»¤åçš„ DataFrame
        """
        if result_include_all or df.empty:
            return df
            
        from src.classification.unified_classifier import UnifiedClassifier
        
        classifier = UnifiedClassifier(data_dir=str(self.data_dir.parent))
        coin_ids = df['coin_id'].unique().tolist()
        
        native_coin_ids = classifier.filter_coins(
            coin_ids=coin_ids,
            exclude_stablecoins=True,
            exclude_wrapped_coins=True,
            use_cache=True
        )
        
        filtered_df = df[df['coin_id'].isin(native_coin_ids)].copy()
        logger.info(f"è¿‡æ»¤åä¿ç•™ {len(native_coin_ids)} ä¸ªåŸç”Ÿå¸ç§")
        
        return filtered_df

    def build_daily_tables(self, force_recalculate: bool = False) -> None:
        """æ„å»ºæ¯æ—¥æ•°æ®è¡¨é›†åˆ

        Args:
            force_recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—æ‰€æœ‰æ•°æ®ï¼Œå¿½ç•¥ç¼“å­˜æ–‡ä»¶
        """
        if not self.coin_data:
            logger.error("è¯·å…ˆè°ƒç”¨ load_coin_data() åŠ è½½æ•°æ®")
            return

        # ç¡®å®šæ—¥æœŸèŒƒå›´
        self._calculate_date_range()
        start_date = self.min_date
        end_date = self.max_date

        # æ£€æŸ¥æ—¥æœŸæœ‰æ•ˆæ€§
        if not start_date or not end_date:
            logger.error("æ— æ³•ç¡®å®šæœ‰æ•ˆçš„æ—¥æœŸèŒƒå›´")
            return

        logger.info(f"æ„å»ºæ¯æ—¥æ•°æ®è¡¨: {start_date} åˆ° {end_date}")

        # ç”Ÿæˆéœ€è¦å¤„ç†çš„æ—¥æœŸåˆ—è¡¨
        date_list = []
        current_date = (
            datetime.strptime(start_date, "%Y-%m-%d").date()
            if isinstance(start_date, str)
            else start_date.date()
        )
        end_date = (
            datetime.strptime(end_date, "%Y-%m-%d").date()
            if isinstance(end_date, str)
            else end_date.date()
        )

        while current_date <= end_date:
            date_list.append(current_date)
            current_date += timedelta(days=1)

        logger.info(f"å°†å¤„ç† {len(date_list)} å¤©çš„æ•°æ®")

        # ä½¿ç”¨å¹¶è¡Œå¤„ç†
        all_daily_data = []
        with ProcessPoolExecutor(
            max_workers=max(1, multiprocessing.cpu_count() - 1)
        ) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_date = {
                executor.submit(self.get_daily_data, date, force_recalculate): date
                for date in date_list
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_date):
                date = future_to_date[future]
                try:
                    daily_data = future.result()
                    if not daily_data.empty:
                        all_daily_data.append(daily_data)
                except Exception as e:
                    logger.error(f"å¤„ç†æ—¥æœŸ {date} æ—¶å‡ºé”™: {e}")

                completed += 1
                if completed % 10 == 0:
                    logger.info(f"å·²å®Œæˆ {completed}/{len(date_list)} å¤©çš„æ•°æ®å¤„ç†")

        logger.info(f"æˆåŠŸå¤„ç† {len(all_daily_data)} å¤©çš„æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ¯æ—¥æ•°æ®åˆ°ä¸€ä¸ªDataFrame
        if all_daily_data:
            merged_daily_data = pd.concat(all_daily_data, ignore_index=True)

            # å¼ºåˆ¶æŒ‰å¸‚å€¼æ’åºå¹¶é‡æ–°åˆ†é…rank
            merged_daily_data = merged_daily_data.sort_values(
                "market_cap", ascending=False
            ).reset_index(drop=True)
            merged_daily_data["rank"] = merged_daily_data.index + 1

            # ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°æ–‡ä»¶
            merged_daily_file = self.output_dir / "merged_daily_data.csv"
            merged_daily_data.to_csv(merged_daily_file, index=False)
            logger.info(f"å·²ä¿å­˜åˆå¹¶åçš„æ¯æ—¥æ•°æ®åˆ°æ–‡ä»¶: {merged_daily_file}")

    def get_data_coverage_analysis(self) -> Dict:
        """åˆ†ææ•°æ®è¦†ç›–æƒ…å†µ"""
        if not self.coin_data:
            return {}

        analysis = {
            "total_coins": len(self.loaded_coins),
            "date_range": {
                "start": str(self.min_date) if self.min_date else None,
                "end": str(self.max_date) if self.max_date else None,
                "total_days": (
                    (self.max_date - self.min_date).days + 1
                    if self.min_date and self.max_date
                    else 0
                ),
            },
            "coin_details": [],
        }

        for coin_id, df in self.coin_data.items():
            coin_start = df["date"].min()
            coin_end = df["date"].max()
            coin_days = len(df)

            analysis["coin_details"].append(
                {
                    "coin_id": coin_id,
                    "start_date": str(coin_start),
                    "end_date": str(coin_end),
                    "data_points": coin_days,
                    "market_cap_avg": df["market_cap"].mean(),
                }
            )

        # æŒ‰æ•°æ®ç‚¹æ•°é‡æ’åº
        analysis["coin_details"].sort(key=lambda x: x["data_points"], reverse=True)

        return analysis

    def find_bitcoin_start_date(self) -> Optional[str]:
        """æ‰¾åˆ°Bitcoinæ•°æ®çš„æœ€æ—©æ—¥æœŸ"""
        if "bitcoin" in self.coin_data:
            btc_data = self.coin_data["bitcoin"]
            return str(btc_data["date"].min())
        return None

    def load_daily_data_from_files(self) -> None:
        """ä»å·²ä¿å­˜çš„æ¯æ—¥æ•°æ®æ–‡ä»¶ä¸­åŠ è½½æ•°æ®"""
        logger.info("ä»å·²ä¿å­˜çš„æ–‡ä»¶ä¸­åŠ è½½æ¯æ—¥æ•°æ®...")

        csv_files = []

        # æ‰«æåˆ†å±‚ç»“æ„
        for year_dir in self.daily_files_dir.iterdir():
            if year_dir.is_dir() and year_dir.name.isdigit():
                for month_dir in year_dir.iterdir():
                    if month_dir.is_dir() and month_dir.name.isdigit():
                        csv_files.extend(list(month_dir.glob("*.csv")))

        # åŒæ—¶æ”¯æŒå¹³é“ºç»“æ„
        csv_files.extend(list(self.daily_files_dir.glob("*.csv")))

        logger.info(f"å‘ç° {len(csv_files)} ä¸ªæ¯æ—¥æ•°æ®æ–‡ä»¶")

        for csv_file in csv_files:
            date_str = csv_file.stem  # æ–‡ä»¶åå°±æ˜¯æ—¥æœŸ
            try:
                daily_df = pd.read_csv(csv_file)
                # è½¬æ¢dateåˆ—çš„æ•°æ®ç±»å‹
                daily_df["date"] = pd.to_datetime(daily_df["date"]).dt.date
                self.daily_cache[date_str] = daily_df
            except Exception as e:
                logger.error(f"åŠ è½½æ¯æ—¥æ•°æ®æ–‡ä»¶å¤±è´¥ {date_str}: {e}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(self.daily_cache)} å¤©çš„æ¯æ—¥æ•°æ®")

    def get_available_daily_dates(self) -> List[str]:
        """è·å–æ‰€æœ‰å·²ç”Ÿæˆæ¯æ—¥æ•°æ®æ–‡ä»¶çš„æ—¥æœŸ"""
        dates = []
        if not self.daily_files_dir.exists():
            return dates

        for year_dir in self.daily_files_dir.iterdir():
            if year_dir.is_dir():
                for month_dir in year_dir.iterdir():
                    if month_dir.is_dir():
                        for file in month_dir.glob("*.csv"):
                            dates.append(file.stem)

        return sorted(dates)

    def get_date_range_summary(self) -> Dict:
        """è·å–æ•°æ®æ—¥æœŸèŒƒå›´çš„æ‘˜è¦ä¿¡æ¯"""
        available_dates = self.get_available_daily_dates()

        if not available_dates:
            return {
                "total_days": 0,
                "start_date": None,
                "end_date": None,
                "coverage": 0.0,
            }

        start_date = available_dates[0]
        end_date = available_dates[-1]

        # è®¡ç®—ç†è®ºæ€»å¤©æ•°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
        theoretical_days = (end_dt - start_dt).days + 1

        # è®¡ç®—è¦†ç›–ç‡
        coverage = (
            len(available_dates) / theoretical_days if theoretical_days > 0 else 0.0
        )

        return {
            "total_days": len(available_dates),
            "theoretical_days": theoretical_days,
            "start_date": start_date,
            "end_date": end_date,
            "coverage": coverage,
        }

    def generate_latest_daily_summary(self, target_days: int = 3, auto_download: bool = True, max_coins: int = 100) -> Dict:
        """
        ç”Ÿæˆæœ€æ–°å‡ å¤©çš„æ¯æ—¥å¸‚åœºæ•°æ®æ±‡æ€»
        
        è¿™ä¸ªæ–¹æ³•ä¼šï¼š
        1. æ£€æŸ¥æ˜¯å¦æœ‰æœ€æ–°çš„ä»·æ ¼æ•°æ®
        2. å¦‚æœæ²¡æœ‰æ•°æ®ä¸” auto_download=Trueï¼Œè‡ªåŠ¨ä¸‹è½½æœ€æ–°æ•°æ®
        3. ä»CSVæ•°æ®æˆ–æ•°æ®åº“ä¸­è¯»å–æœ€æ–°çš„ä»·æ ¼æ•°æ®
        4. ç”ŸæˆæŒ‡å®šå¤©æ•°çš„æ¯æ—¥æ±‡æ€»æ–‡ä»¶
        5. åŒæ—¶æ›´æ–°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        
        Args:
            target_days: è¦ç”Ÿæˆçš„æœ€è¿‘å¤©æ•°ï¼Œé»˜è®¤3å¤©
            auto_download: æ˜¯å¦åœ¨æ²¡æœ‰æ•°æ®æ—¶è‡ªåŠ¨ä¸‹è½½ï¼Œé»˜è®¤True
            max_coins: è‡ªåŠ¨ä¸‹è½½æ—¶çš„æœ€å¤§å¸ç§æ•°ï¼Œé»˜è®¤100
            
        Returns:
            ç”Ÿæˆç»“æœæ‘˜è¦
        """
        from datetime import datetime, timedelta
        import logging
        
        logger.info(f"å¼€å§‹ç”Ÿæˆæœ€æ–° {target_days} å¤©çš„æ¯æ—¥æ±‡æ€»æ•°æ®...")
        
        results = {
            "processed_dates": [],
            "skipped_dates": [],
            "failed_dates": [],
            "downloaded_dates": [],
            "success_count": 0,
            "total_count": target_days,
            "auto_download_triggered": False
        }
        
        # ç”Ÿæˆç›®æ ‡æ—¥æœŸåˆ—è¡¨ï¼ˆæœ€è¿‘çš„å‡ å¤©ï¼‰
        today = datetime.now().date()
        target_dates = []
        
        for i in range(target_days):
            target_date = today - timedelta(days=i)
            target_dates.append(target_date)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½æ•°æ®
        if auto_download:
            self._check_and_download_missing_data(target_dates, max_coins, results)
        
        for target_date in target_dates:
            date_str = target_date.strftime('%Y-%m-%d')
            
            try:
                logger.info(f"å¤„ç†æ—¥æœŸ: {date_str}")
                
                # é‡æ–°åŠ è½½å¸ç§æ•°æ®ï¼ˆå¦‚æœæœ‰æ–°ä¸‹è½½çš„ï¼‰
                if results["auto_download_triggered"] and not self.coin_data:
                    logger.info("æ£€æµ‹åˆ°æ–°ä¸‹è½½çš„æ•°æ®ï¼Œé‡æ–°åŠ è½½å¸ç§æ•°æ®...")
                    self.load_coin_data()
                
                # è®¡ç®—è¿™ä¸€å¤©çš„æ•°æ®
                daily_df = self._compute_daily_data(target_date)
                
                if daily_df.empty:
                    logger.warning(f"{date_str}: æ²¡æœ‰å¯ç”¨æ•°æ®")
                    results["skipped_dates"].append(date_str)
                    continue
                
                # ä¿å­˜CSVæ–‡ä»¶
                output_file = self._get_daily_file_path(target_date)
                output_file.parent.mkdir(parents=True, exist_ok=True)
                
                daily_df.to_csv(output_file, index=False)
                logger.info(f"{date_str}: ä¿å­˜äº† {len(daily_df)} æ¡è®°å½•åˆ° {output_file}")
                
                # å¦‚æœå¯ç”¨æ•°æ®åº“ï¼ŒåŒæ—¶æ›´æ–°æ•°æ®åº“
                if self.use_database and hasattr(self, 'db_manager') and self.db_manager:
                    try:
                        self._sync_daily_data_to_database(daily_df, date_str)
                        logger.debug(f"{date_str}: æ•°æ®å·²åŒæ­¥åˆ°æ•°æ®åº“")
                    except Exception as db_error:
                        logger.warning(f"{date_str}: æ•°æ®åº“åŒæ­¥å¤±è´¥: {db_error}")
                
                results["processed_dates"].append(date_str)
                results["success_count"] += 1
                
            except Exception as e:
                logger.error(f"{date_str}: å¤„ç†å¤±è´¥: {e}")
                results["failed_dates"].append(date_str)
        
        # ç”Ÿæˆç»“æœæ‘˜è¦
        success_rate = (results["success_count"] / target_days * 100) if target_days > 0 else 0
        logger.info(f"æ¯æ—¥æ±‡æ€»ç”Ÿæˆå®Œæˆ: {results['success_count']}/{target_days} æˆåŠŸ ({success_rate:.1f}%)")
        
        if results["processed_dates"]:
            logger.info(f"æˆåŠŸå¤„ç†æ—¥æœŸ: {', '.join(results['processed_dates'])}")
        if results["failed_dates"]:
            logger.warning(f"å¤±è´¥æ—¥æœŸ: {', '.join(results['failed_dates'])}")
        if results["skipped_dates"]:
            logger.info(f"è·³è¿‡æ—¥æœŸ: {', '.join(results['skipped_dates'])}")
        
        return results
    
    def _check_and_download_missing_data(self, target_dates: List, max_coins: int, results: Dict) -> None:
        """
        æ£€æŸ¥å¹¶ä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        
        Args:
            target_dates: ç›®æ ‡æ—¥æœŸåˆ—è¡¨
            max_coins: æœ€å¤§ä¸‹è½½å¸ç§æ•°
            results: ç»“æœå­—å…¸ï¼Œç”¨äºè®°å½•ä¸‹è½½çŠ¶æ€
        """
        from datetime import datetime, timedelta
        
        logger.info("æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½æœ€æ–°æ•°æ®...")
        
        # æ£€æŸ¥æœ€è¿‘çš„æ•°æ®æ˜¯å¦å­˜åœ¨
        missing_recent_data = False
        
        # æ£€æŸ¥ä»Šå¤©å’Œæ˜¨å¤©çš„æ•°æ®
        today = datetime.now().date()
        yesterday = today - timedelta(days=1)
        
        for check_date in [today, yesterday]:
            date_str = check_date.strftime('%Y-%m-%d')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¸ç§çš„æ•°æ®æ–‡ä»¶åŒ…å«è¿™ä¸ªæ—¥æœŸ
            has_recent_data = False
            
            # å¿«é€Ÿæ£€æŸ¥ï¼šçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•CSVæ–‡ä»¶çš„æœ€åä¿®æ”¹æ—¶é—´æ˜¯ä»Šå¤©æˆ–æ˜¨å¤©
            if self.data_dir.exists():
                import os
                for csv_file in self.data_dir.glob("*.csv"):
                    if csv_file.exists():
                        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(csv_file)).date()
                        if file_mtime >= yesterday:
                            # è¿›ä¸€æ­¥æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«æœ€æ–°æ—¥æœŸ
                            try:
                                import pandas as pd
                                df = pd.read_csv(csv_file, nrows=5)  # åªè¯»å‰å‡ è¡Œæ£€æŸ¥
                                if not df.empty and 'timestamp' in df.columns:
                                    df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
                                    df['date'] = pd.to_datetime(df['timestamp'], unit='ms').dt.strftime('%Y-%m-%d')
                                    if date_str in df['date'].values:
                                        has_recent_data = True
                                        break
                            except Exception:
                                continue
            
            if not has_recent_data:
                logger.info(f"æœªæ‰¾åˆ° {date_str} çš„æ•°æ®")
                missing_recent_data = True
                break
            else:
                logger.info(f"æ‰¾åˆ° {date_str} çš„æ•°æ®")
        
        # å¦‚æœç¼ºå°‘æœ€è¿‘çš„æ•°æ®ï¼Œè§¦å‘ä¸‹è½½
        if missing_recent_data:
            logger.info(f"æ£€æµ‹åˆ°ç¼ºå°‘æœ€æ–°æ•°æ®ï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½å‰ {max_coins} ä¸ªå¸ç§çš„æœ€æ–°ä»·æ ¼...")
            results["auto_download_triggered"] = True
            
            try:
                # å¯¼å…¥ä¸‹è½½å™¨
                from ..downloaders.batch_downloader import create_batch_downloader
                
                # åˆ›å»ºä¸‹è½½å™¨ï¼ˆå¯ç”¨æ•°æ®åº“å†™å…¥ï¼‰
                downloader = create_batch_downloader(enable_database=True)
                
                # ä¸‹è½½æœ€æ–°æ•°æ®ï¼ˆæœ€è¿‘7å¤©ï¼‰
                download_results = downloader.download_batch(
                    top_n=max_coins,
                    days="7",  # ä¸‹è½½æœ€è¿‘7å¤©æ•°æ®ç¡®ä¿åŒ…å«æœ€æ–°ä»·æ ¼
                    force_update=True  # å¼ºåˆ¶æ›´æ–°
                )
                
                # ç»Ÿè®¡ä¸‹è½½ç»“æœ
                success_count = sum(1 for status in download_results.values() if status == "success")
                logger.info(f"è‡ªåŠ¨ä¸‹è½½å®Œæˆ: {success_count}/{max_coins} ä¸ªå¸ç§æˆåŠŸ")
                
                if success_count > 0:
                    # è®°å½•ä¸‹è½½çš„æ—¥æœŸ
                    for date in target_dates:
                        results["downloaded_dates"].append(date.strftime('%Y-%m-%d'))
                    
                    # æ¸…ç©ºå¸ç§æ•°æ®ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
                    self.coin_data.clear()
                    self.loaded_coins.clear()
                    logger.info("å·²æ¸…ç©ºæ•°æ®ç¼“å­˜ï¼Œå°†é‡æ–°åŠ è½½æœ€æ–°æ•°æ®")
                else:
                    logger.warning("è‡ªåŠ¨ä¸‹è½½æœªè·å–åˆ°ä»»ä½•æ•°æ®")
                    
            except Exception as e:
                logger.error(f"è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
                results["auto_download_triggered"] = False
        else:
            logger.info("å‘ç°æœ€æ–°æ•°æ®ï¼Œæ— éœ€ä¸‹è½½")
    
    def _sync_daily_data_to_database(self, daily_df: pd.DataFrame, date_str: str) -> None:
        """
        å°†æ¯æ—¥æ±‡æ€»æ•°æ®åŒæ­¥åˆ°æ•°æ®åº“
        
        Args:
            daily_df: æ¯æ—¥æ±‡æ€»æ•°æ®
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
        """
        if not hasattr(self, 'db_manager') or not self.db_manager:
            return
            
        try:
            # å°†æ¯æ—¥æ±‡æ€»æ•°æ®è½¬æ¢ä¸ºæ•°æ®åº“è®°å½•æ ¼å¼
            for _, row in daily_df.iterrows():
                coin_id = row.get('coin_id', '')
                if not coin_id:
                    continue
                    
                record = {
                    'timestamp': int(datetime.strptime(date_str, '%Y-%m-%d').timestamp() * 1000),
                    'price': row.get('price'),
                    'volume': row.get('volume'),
                    'market_cap': row.get('market_cap')
                }
                
                # æ’å…¥åˆ°æ•°æ®åº“
                self.db_manager.insert_coin_price_data(coin_id, [record])
                    
        except Exception as e:
            logger.error(f"æ•°æ®åº“åŒæ­¥å¤±è´¥: {e}")
            raise

    def _get_daily_file_path(self, target_date: date) -> Path:
        """æ ¹æ®æ—¥æœŸè·å–æ¯æ—¥æ•°æ®æ–‡ä»¶çš„è·¯å¾„"""
        # ç¡®ä¿è¾“å…¥æ˜¯ date å¯¹è±¡
        if isinstance(target_date, datetime):
            target_date = target_date.date()

        date_str = target_date.strftime("%Y-%m-%d")
        year = target_date.strftime("%Y")
        month = target_date.strftime("%m")

        # åˆ›å»ºå¹´æœˆç›®å½•
        year_month_dir = self.daily_files_dir / year / month
        year_month_dir.mkdir(parents=True, exist_ok=True)

        return year_month_dir / f"{date_str}.csv"

    def _compute_daily_data(self, target_date: date) -> pd.DataFrame:
        """åœ¨å†…å­˜ä¸­è®¡ç®—æŒ‡å®šæ—¥æœŸçš„èšåˆæ•°æ®"""
        daily_records = []
        target_date_str = target_date.strftime("%Y-%m-%d")
        logger.info(
            f"å¼€å§‹è®¡ç®— {target_date_str} çš„æ•°æ®ï¼Œéå† {len(self.coin_data)} ä¸ªå·²åŠ è½½çš„å¸ç§..."
        )

        # å¦‚æœå¸ç§æ•°é‡è¶³å¤Ÿå¤šï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
        if len(self.coin_data) > 100:
            # ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€éƒ¨åˆ†å¸ç§
            with ProcessPoolExecutor(
                max_workers=max(1, multiprocessing.cpu_count() - 1)
            ) as executor:
                # å‡†å¤‡ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å¤„ç†ä¸€æ‰¹å¸ç§
                coin_batches = self._split_coins_into_batches(
                    list(self.coin_data.items()), 10
                )
                futures = []

                for batch in coin_batches:
                    futures.append(
                        executor.submit(
                            self._process_coin_batch, batch, target_date_str
                        )
                    )

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    try:
                        batch_results = future.result()
                        daily_records.extend(batch_results)
                    except Exception as e:
                        logger.error(f"å¤„ç†å¸ç§æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
        else:
            # å¸ç§æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨å•çº¿ç¨‹å¤„ç†
            for coin_id, df in self.coin_data.items():
                if df.empty:
                    continue

                # ç­›é€‰ç‰¹å®šæ—¥æœŸçš„æ•°æ®
                day_data = df[df["date"] == target_date_str]

                if not day_data.empty:
                    # é€šå¸¸æ¯å¤©åªæœ‰ä¸€ä¸ªè®°å½•ï¼Œä½†ä¸ºé˜²ä¸‡ä¸€ï¼Œå–ç¬¬ä¸€ä¸ª
                    record = day_data.iloc[0].to_dict()
                    daily_records.append(record)
                    logger.debug(f"æ‰¾åˆ° {coin_id} åœ¨ {target_date_str} çš„æ•°æ®ã€‚")
                else:
                    logger.debug(f"æœªæ‰¾åˆ° {coin_id} åœ¨ {target_date_str} çš„æ•°æ®ã€‚")

        if not daily_records:
            logger.warning(f"åœ¨ {target_date_str} æœªæ‰¾åˆ°ä»»ä½•å¸ç§çš„æ•°æ®ã€‚")
            return pd.DataFrame()

        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        final_df = pd.DataFrame(daily_records)
        logger.info(f"ä¸º {target_date_str} èšåˆäº† {len(final_df)} ä¸ªå¸ç§çš„æ•°æ®ã€‚")

        # æ·»åŠ æ’å
        if "market_cap" in final_df.columns:
            final_df = final_df.sort_values("market_cap", ascending=False)
            final_df = final_df.reset_index(drop=True)
            final_df["rank"] = final_df.index + 1

        return final_df

    @staticmethod
    def _split_coins_into_batches(coins, batch_size):
        """å°†å¸ç§åˆ—è¡¨åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†"""
        for i in range(0, len(coins), batch_size):
            yield coins[i : i + batch_size]

    @staticmethod
    def _process_coin_batch(coin_batch, target_date_str):
        """å¤„ç†ä¸€æ‰¹å¸ç§çš„æ•°æ®ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
        batch_results = []
        for coin_id, df in coin_batch:
            if df.empty:
                continue

            # ç­›é€‰ç‰¹å®šæ—¥æœŸçš„æ•°æ®
            day_data = df[df["date"] == target_date_str]

            if not day_data.empty:
                record = day_data.iloc[0].to_dict()
                batch_results.append(record)

        return batch_results

    def build_daily_market_summary(
        self, output_path: Optional[str] = None
    ) -> pd.DataFrame:
        """
        ç”Ÿæˆæ¯æ—¥å¸‚åœºæ‘˜è¦æ•°æ®

        éå†æ‰€æœ‰æ¯æ—¥æ•°æ®æ–‡ä»¶ï¼Œè®¡ç®—å¸‚åœºæ‘˜è¦ç»Ÿè®¡æ•°æ®

        Args:
            output_path: å¯é€‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜CSVæ–‡ä»¶

        Returns:
            åŒ…å«æ¯æ—¥å¸‚åœºæ‘˜è¦çš„DataFrame
        """
        if output_path is None:
            output_file = self.output_dir / "daily_summary.csv"
        else:
            output_file = Path(output_path)

        daily_files_dir = self.output_dir / "daily_files"

        if not daily_files_dir.exists():
            logger.error(f"æ¯æ—¥æ•°æ®ç›®å½•ä¸å­˜åœ¨: {daily_files_dir}")
            return pd.DataFrame()

        # æŸ¥æ‰¾æ‰€æœ‰æ—¥åº¦æ•°æ®æ–‡ä»¶
        daily_files = sorted(list(daily_files_dir.glob("*/*/*.csv")))

        if not daily_files:
            logger.warning(f"åœ¨ {daily_files_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ—¥åº¦æ•°æ®æ–‡ä»¶")
            return pd.DataFrame()

        logger.info(f"æ‰¾åˆ° {len(daily_files)} ä¸ªæ—¥åº¦æ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹ç”Ÿæˆæ‘˜è¦...")

        summary_data = []

        for file_path in daily_files:
            try:
                # ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ
                date_str = file_path.stem

                df = pd.read_csv(file_path)

                # è·³è¿‡ç©ºæ–‡ä»¶
                if df.empty:
                    continue

                coin_count = len(df)
                total_market_cap = df["market_cap"].sum()
                total_volume = df["volume"].sum()

                # è®¡ç®—å¹³å‡å€¼ï¼Œé¿å…é™¤ä»¥é›¶
                avg_market_cap = total_market_cap / coin_count if coin_count > 0 else 0
                avg_volume = total_volume / coin_count if coin_count > 0 else 0

                summary_data.append(
                    {
                        "date": date_str,
                        "coin_count": coin_count,
                        "total_market_cap": total_market_cap,
                        "total_volume": total_volume,
                        "avg_market_cap": avg_market_cap,
                        "avg_volume": avg_volume,
                    }
                )

            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue

        if not summary_data:
            logger.warning("æ²¡æœ‰ç”Ÿæˆä»»ä½•æ‘˜è¦æ•°æ®")
            return pd.DataFrame()

        # åˆ›å»ºDataFrameå¹¶æŒ‰æ—¥æœŸæ’åº
        summary_df = pd.DataFrame(summary_data)
        summary_df["date"] = pd.to_datetime(summary_df["date"])
        summary_df = summary_df.sort_values(by="date").reset_index(drop=True)

        # æ ¼å¼åŒ–æ—¥æœŸåˆ—
        summary_df["date"] = summary_df["date"].dt.strftime("%Y-%m-%d")

        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        summary_df.to_csv(output_file, index=False)
        logger.info(f"æ¯æ—¥å¸‚åœºæ‘˜è¦å·²ä¿å­˜åˆ°: {output_file}")
        logger.info(f"æ€»å…±å¤„ç†äº† {len(summary_df)} å¤©çš„æ•°æ®")

        return summary_df

    def reorder_daily_files_by_market_cap(
        self,
        dry_run: bool = False,
        max_workers: int = 8,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Tuple[int, int]:
        """
        æŒ‰å¸‚å€¼é‡æ’åºæ¯æ—¥æ–‡ä»¶å¹¶é‡æ–°åˆ†é…rankå­—æ®µ

        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DDï¼Œå¯é€‰)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DDï¼Œå¯é€‰)

        Returns:
            Tuple[int, int]: (æˆåŠŸå¤„ç†æ•°é‡, æ€»æ–‡ä»¶æ•°é‡)
        """
        import glob
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from datetime import datetime

        self.logger.info(f"å¼€å§‹é‡æ’åºæ¯æ—¥æ–‡ä»¶ï¼Œdry_run={dry_run}")

        # è·å–ç›®æ ‡æ–‡ä»¶åˆ—è¡¨
        if start_date and end_date:
            target_files = self._find_files_by_date_range(start_date, end_date)
            self.logger.info(
                f"æŒ‰æ—¥æœŸèŒƒå›´ç­›é€‰ï¼š{start_date} åˆ° {end_date}ï¼Œæ‰¾åˆ° {len(target_files)} ä¸ªæ–‡ä»¶"
            )
        else:
            target_files = self._find_all_daily_files()
            self.logger.info(f"å¤„ç†æ‰€æœ‰æ¯æ—¥æ–‡ä»¶ï¼Œæ‰¾åˆ° {len(target_files)} ä¸ªæ–‡ä»¶")

        if not target_files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
            return 0, 0

        successful = 0
        failed = 0

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(
                    self._process_single_file_reorder, file_path, dry_run
                ): file_path
                for file_path in target_files
            }

            # ç­‰å¾…å®Œæˆå¹¶æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    success = future.result()
                    if success:
                        successful += 1
                        if not dry_run:
                            self.logger.debug(
                                f"å·²é‡æ’åº: {os.path.basename(file_path)}"
                            )
                    else:
                        failed += 1
                        self.logger.warning(
                            f"é‡æ’åºå¤±è´¥: {os.path.basename(file_path)}"
                        )
                except Exception as e:
                    failed += 1
                    self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

        self.logger.info(
            f"é‡æ’åºå®Œæˆ: æˆåŠŸ {successful}, å¤±è´¥ {failed}, æ€»è®¡ {len(target_files)}"
        )
        return successful, len(target_files)

    def _process_single_file_reorder(
        self, file_path: str, dry_run: bool = False
    ) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„é‡æ’åº

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            df = pd.read_csv(file_path)

            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            if "market_cap" not in df.columns or "rank" not in df.columns:
                self.logger.warning(
                    f"æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…è¦å­—æ®µ (market_cap æˆ– rank)"
                )
                return False

            # å¸‚å€¼å­—æ®µé™åºæ’åº
            df_sorted = df.sort_values(by="market_cap", ascending=False)
            # é‡æ–°èµ‹å€¼æ’å
            df_sorted["rank"] = range(1, len(df_sorted) + 1)

            if dry_run:
                self.logger.info(
                    f"[DRY-RUN] {os.path.basename(file_path)} é‡æ’åºé¢„è§ˆ (å‰3è¡Œ):"
                )
                self.logger.info(f"\n{df_sorted.head(3).to_string()}")
            else:
                df_sorted.to_csv(file_path, index=False)

            return True

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return False

    def _find_all_daily_files(self) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰æ¯æ—¥æ±‡æ€»æ–‡ä»¶"""
        files = []
        daily_files_dir = self.daily_files_dir

        if not os.path.exists(daily_files_dir):
            self.logger.warning(f"æ¯æ—¥æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {daily_files_dir}")
            return files

        # ä½¿ç”¨globæ¨¡å¼åŒ¹é…æ‰€æœ‰CSVæ–‡ä»¶
        pattern = os.path.join(daily_files_dir, "*", "*", "*.csv")
        files = glob.glob(pattern)

        return sorted(files)

    def _find_files_by_date_range(self, start_date: str, end_date: str) -> List[str]:
        """
        æ ¹æ®æ—¥æœŸèŒƒå›´æŸ¥æ‰¾æ–‡ä»¶

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            List[str]: ç¬¦åˆæ—¥æœŸèŒƒå›´çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        from datetime import datetime, timedelta

        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        except ValueError as e:
            self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
            return []

        files = []
        current_dt = start_dt

        while current_dt <= end_dt:
            date_str = current_dt.strftime("%Y-%m-%d")
            year = current_dt.strftime("%Y")
            month = current_dt.strftime("%m")

            file_path = os.path.join(
                self.daily_files_dir, year, month, f"{date_str}.csv"
            )

            if os.path.exists(file_path):
                files.append(file_path)

            current_dt += timedelta(days=1)

        return files


def create_daily_aggregator(
    data_dir: str = "data/coins", 
    output_dir: str = "data/daily", 
    use_database: bool = True
) -> DailyDataAggregator:
    """åˆ›å»ºæ¯æ—¥æ•°æ®èšåˆå™¨å®ä¾‹
    
    Args:
        data_dir: åŸå§‹CSVæ•°æ®ç›®å½•
        output_dir: èšåˆåæ•°æ®è¾“å‡ºç›®å½•
        use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆé»˜è®¤å¯ç”¨ä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰
    """
    logger.info(f"æ¯æ—¥æ•°æ®èšåˆå™¨åˆå§‹åŒ–, æ•°æ®æº: '{data_dir}', è¾“å‡ºåˆ°: '{output_dir}', æ•°æ®åº“æ¨¡å¼: {use_database}")
    return DailyDataAggregator(data_dir, output_dir, use_database=use_database)
