"""
å¸‚å€¼åŠ æƒæŒ‡æ•°è®¡ç®—å™¨

åŸºäºå¸‚å€¼åŠ æƒæ–¹å¼è®¡ç®—åŒºå—é“¾èµ„äº§æŒ‡æ•°

âš ï¸ é‡è¦æ•°æ®è¯´æ˜:
- ä½¿ç”¨çš„å¸‚å€¼ä¸ºæµé€šå¸‚å€¼ (Circulating Market Cap)ï¼Œéå®Œå…¨ç¨€é‡Šå¸‚å€¼
- æµé€šå¸‚å€¼ = å½“å‰ä»·æ ¼ Ã— æµé€šä¾›åº”é‡
- è¿™æ˜¯é‡‘èå¸‚åœºå’ŒæŒ‡æ•°ç¼–åˆ¶çš„æ ‡å‡†åšæ³•ï¼Œæ›´èƒ½åæ˜ çœŸå®å¯äº¤æ˜“ä»·å€¼
- æ•°æ®æ¥æº: CoinGecko API çš„å†å²å›¾è¡¨æ•°æ®
"""

import logging
import os
import sys
from datetime import date, datetime
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(
    0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)

from src.classification.unified_classifier import UnifiedClassifier
from src.downloaders.daily_aggregator import DailyDataAggregator


class MarketCapWeightedIndexCalculator:
    """å¸‚å€¼åŠ æƒæŒ‡æ•°è®¡ç®—å™¨"""

    def __init__(
        self,
        data_dir: str = "data/coins",
        daily_output_dir: str = "data/daily",
        exclude_stablecoins: bool = True,
        exclude_wrapped_coins: bool = True,
        force_rebuild: bool = False,
        use_database: bool = True,
    ):
        """
        åˆå§‹åŒ–æŒ‡æ•°è®¡ç®—å™¨

        Args:
            data_dir: åŸå§‹ä»·æ ¼æ•°æ®ç›®å½•è·¯å¾„
            daily_output_dir: æ¯æ—¥æ±‡æ€»æ•°æ®çš„è¾“å‡ºç›®å½•
            exclude_stablecoins: æ˜¯å¦æ’é™¤ç¨³å®šå¸
            exclude_wrapped_coins: æ˜¯å¦æ’é™¤åŒ…è£…å¸
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ¯æ—¥æ•°æ®æ–‡ä»¶
            use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆæ¨èå¼€å¯ä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰

        æ³¨æ„ï¼š
        - æ ¸å¿ƒæ•°æ®æ¥æºï¼š{daily_output_dir}/daily_files/
        - æ•°æ®åº“æ¨¡å¼ï¼šå¤æ‚æŸ¥è¯¢æ€§èƒ½æå‡10-100å€
        """
        self.data_dir = Path(data_dir)
        self.daily_output_dir = Path(daily_output_dir)
        self.exclude_stablecoins = exclude_stablecoins
        self.exclude_wrapped_coins = exclude_wrapped_coins
        self.force_rebuild = force_rebuild
        self.use_database = use_database

        # åˆå§‹åŒ–æ¯æ—¥æ•°æ®èšåˆå™¨ - æ ¸å¿ƒæ•°æ®æº
        self.daily_aggregator = DailyDataAggregator(
            data_dir=str(self.data_dir), 
            output_dir=str(self.daily_output_dir),
            use_database=use_database
        )

        # åˆå§‹åŒ–ç»Ÿä¸€åˆ†ç±»å™¨ (æ›¿ä»£åŸæœ‰çš„ä¸¤ä¸ªåˆ†ç¦»åˆ†ç±»å™¨)
        self.classifier = UnifiedClassifier()

        # ä¿æŒå‘åå…¼å®¹ï¼šè®°å½•è¿‡æ»¤è®¾ç½®
        self.exclude_stablecoins = exclude_stablecoins
        self.exclude_wrapped_coins = exclude_wrapped_coins

        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)

    def _load_coin_data(self, coin_id: str) -> Optional[pd.DataFrame]:
        """
        åŠ è½½å•ä¸ªå¸ç§çš„ä»·æ ¼æ•°æ®

        Args:
            coin_id: å¸ç§IDï¼ˆæ–‡ä»¶åä¸å«.csvåç¼€ï¼‰

        Returns:
            åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrameï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›None
        """
        csv_path = self.data_dir / f"{coin_id}.csv"
        if not csv_path.exists():
            return None

        try:
            df = pd.read_csv(csv_path)
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸ
            df["date"] = pd.to_datetime(df["timestamp"], unit="ms").dt.date
            df = df.sort_values("date")
            return df
        except Exception as e:
            self.logger.warning(f"è¯»å– {coin_id} æ•°æ®å¤±è´¥: {e}")
            return None

    def _get_available_coins(self) -> List[str]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„å¸ç§IDåˆ—è¡¨

        Returns:
            å¸ç§IDåˆ—è¡¨
        """
        coins = []
        for csv_file in self.data_dir.glob("*.csv"):
            coin_id = csv_file.stem

            # ä½¿ç”¨ç»Ÿä¸€åˆ†ç±»å™¨è¿›è¡Œè¿‡æ»¤ (æ€§èƒ½ä¼˜åŒ–ï¼šä¸€æ¬¡è°ƒç”¨è·å–æ‰€æœ‰åˆ†ç±»ä¿¡æ¯)
            if self.exclude_stablecoins or self.exclude_wrapped_coins:
                try:
                    classification_result = self.classifier.classify_coin(coin_id)

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤ç¨³å®šå¸
                    if self.exclude_stablecoins and classification_result.is_stablecoin:
                        continue

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤åŒ…è£…å¸
                    if (
                        self.exclude_wrapped_coins
                        and classification_result.is_wrapped_coin
                    ):
                        continue
                except Exception:
                    # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œä¸æ’é™¤
                    pass

            coins.append(coin_id)

        return coins

    def _get_daily_data_cached(self, target_date: date) -> pd.DataFrame:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„æ¯æ—¥æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            å½“æ—¥æ‰€æœ‰å¸ç§æ•°æ®çš„DataFrame
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç¼“å­˜ä¸­
        cache_key = target_date.isoformat()
        if hasattr(self, "_daily_cache") and cache_key in self._daily_cache:
            return self._daily_cache[cache_key]

        # åˆå§‹åŒ–ç¼“å­˜
        if not hasattr(self, "_daily_cache"):
            self._daily_cache = {}

        # ğŸ”§ ç¡®å®šæ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ·æ–°å’Œæ•°æ®åº“ä¼˜åŒ–è®¾ç½®
        # force_rebuild åªåœ¨ç¬¬ä¸€æ¬¡è·å–ç‰¹å®šæ—¥æœŸæ•°æ®æ—¶ç”Ÿæ•ˆ
        force_refresh = self.force_rebuild and cache_key not in self._daily_cache
        
        # ğŸ¯ å…³é”®ä¿®å¤: æ€»æ˜¯è·å–æ‰€æœ‰æ•°æ® (result_include_all=True)
        # åŸå› : æŒ‡æ•°è®¡ç®—å™¨è‡ªå·±è´Ÿè´£è¿‡æ»¤ï¼Œé¿å…åœ¨ç¼“å­˜å±‚é¢çš„é…ç½®ä¸ä¸€è‡´
        # å¥½å¤„: ç¡®ä¿ä¸åŒé…ç½®çš„è®¡ç®—å™¨å®ä¾‹èƒ½è·å¾—ä¸€è‡´çš„åŸºç¡€æ•°æ®
        daily_df = self.daily_aggregator.get_daily_data(
            target_date, 
            force_refresh=force_refresh, 
            result_include_all=True,
            prefer_database=self.use_database  # ğŸš€ æ–°å¢ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“
        )

        # ç¼“å­˜ç»“æœ (ç¼“å­˜çš„æ˜¯å®Œæ•´æ•°æ®ï¼Œè¿‡æ»¤ç”±è®¡ç®—å™¨è´Ÿè´£)
        self._daily_cache[cache_key] = daily_df
        return daily_df

    def _get_daily_market_caps(self, target_date: date) -> Dict[str, float]:
        """
        è·å–æŒ‡å®šæ—¥æœŸæ‰€æœ‰å¸ç§çš„å¸‚å€¼

        ä½¿ç”¨æ¯æ—¥æ±‡æ€»æ•°æ®æºè€Œéå•ç‹¬çš„å¸ç§æ–‡ä»¶

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            å¸ç§IDåˆ°å¸‚å€¼çš„æ˜ å°„å­—å…¸

        âš ï¸ é‡è¦: è¿”å›çš„æ˜¯æµé€šå¸‚å€¼ (Circulating Market Cap)
        - æµé€šå¸‚å€¼ = å½“å‰ä»·æ ¼ Ã— æµé€šä¾›åº”é‡
        - ç”¨äºæŒ‡æ•°æƒé‡è®¡ç®—å’Œæ’åç­›é€‰
        - ç¬¦åˆä¼ ç»Ÿé‡‘èæŒ‡æ•°ç¼–åˆ¶æ ‡å‡†
        """
        try:
            # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®è·å–æ–¹æ³•
            daily_df = self._get_daily_data_cached(target_date)

            if daily_df.empty:
                self.logger.warning(f"æ—¥æœŸ {target_date} æ²¡æœ‰å¯ç”¨çš„æ¯æ—¥æ±‡æ€»æ•°æ®")
                return {}

            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
            if (
                "coin_id" not in daily_df.columns
                or "market_cap" not in daily_df.columns
            ):
                self.logger.error(f"æ—¥æœŸ {target_date} çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘å¿…è¦åˆ—")
                return {}

            # è¿‡æ»¤ç¨³å®šå¸å’ŒåŒ…è£…å¸
            filtered_df = self._filter_coins(daily_df)

            # è½¬æ¢ä¸ºå­—å…¸ï¼Œåªä¿ç•™æœ‰æ•ˆçš„å¸‚å€¼æ•°æ®
            market_caps = {}
            for _, row in filtered_df.iterrows():
                market_cap = row["market_cap"]
                if pd.notna(market_cap) and market_cap > 0:
                    market_caps[row["coin_id"]] = float(market_cap)

            self.logger.debug(
                f"æ—¥æœŸ {target_date}: è·å–åˆ° {len(market_caps)} ä¸ªå¸ç§çš„å¸‚å€¼æ•°æ®"
            )
            return market_caps

        except Exception as e:
            self.logger.error(f"è·å–æ—¥æœŸ {target_date} çš„å¸‚å€¼æ•°æ®å¤±è´¥: {e}")
            return {}

    def _select_top_coins(self, market_caps: Dict[str, float], top_n: int) -> List[str]:
        """
        æ ¹æ®å¸‚å€¼é€‰æ‹©å‰Nåå¸ç§

        Args:
            market_caps: å¸ç§å¸‚å€¼å­—å…¸
            top_n: é€‰æ‹©æ•°é‡

        Returns:
            æŒ‰å¸‚å€¼æ’åºçš„å‰Nåå¸ç§IDåˆ—è¡¨
        """
        sorted_coins = sorted(market_caps.items(), key=lambda x: x[1], reverse=True)
        return [coin_id for coin_id, _ in sorted_coins[:top_n]]

    def _calculate_weights(
        self, coin_ids: List[str], market_caps: Dict[str, float]
    ) -> Dict[str, float]:
        """
        âš ï¸ å·²åºŸå¼ƒï¼šè®¡ç®—å„å¸ç§çš„æƒé‡

        è¿™ä¸ªæ–¹æ³•åœ¨æ—§çš„ä»·æ ¼åŠ æƒç®—æ³•ä¸­ä½¿ç”¨ï¼Œç°åœ¨çš„æ­£ç¡®ç®—æ³•ç›´æ¥æ¯”è¾ƒå¸‚å€¼ï¼Œæ— éœ€æƒé‡è®¡ç®—ã€‚
        ä¿ç•™æ­¤æ–¹æ³•ä»…ä¸ºå‘åå…¼å®¹ï¼Œä½†æ–°ä»£ç ä¸åº”ä½¿ç”¨ã€‚

        Args:
            coin_ids: æˆåˆ†å¸ç§IDåˆ—è¡¨
            market_caps: å¸ç§å¸‚å€¼å­—å…¸

        Returns:
            å¸ç§IDåˆ°æƒé‡çš„æ˜ å°„å­—å…¸
        """
        total_market_cap = sum(market_caps[coin_id] for coin_id in coin_ids)

        weights = {}
        for coin_id in coin_ids:
            weights[coin_id] = market_caps[coin_id] / total_market_cap

        return weights

    def _get_coin_price(self, coin_id: str, target_date: date) -> Optional[float]:
        """
        è·å–æŒ‡å®šå¸ç§åœ¨æŒ‡å®šæ—¥æœŸçš„ä»·æ ¼

        ä½¿ç”¨æ¯æ—¥æ±‡æ€»æ•°æ®æºè€Œéå•ç‹¬çš„å¸ç§æ–‡ä»¶

        Args:
            coin_id: å¸ç§ID
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            ä»·æ ¼ï¼Œå¦‚æœæ— æ•°æ®è¿”å›None
        """
        try:
            # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®è·å–æ–¹æ³•
            daily_df = self._get_daily_data_cached(target_date)

            if daily_df.empty:
                return None

            coin_data = daily_df[daily_df["coin_id"] == coin_id]

            if coin_data.empty:
                return None

            price = coin_data.iloc[0]["price"]
            return float(price) if pd.notna(price) and price > 0 else None

        except Exception as e:
            self.logger.warning(f"è·å– {coin_id} åœ¨ {target_date} çš„ä»·æ ¼å¤±è´¥: {e}")
            return None

    def _filter_coins(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¿‡æ»¤ç¨³å®šå¸å’ŒåŒ…è£…å¸

        Args:
            df: åŒ…å«å¸ç§æ•°æ®çš„DataFrame

        Returns:
            è¿‡æ»¤åçš„DataFrame
        """
        if df.empty or "coin_id" not in df.columns:
            return df

        filtered_df = df.copy()

        # ä½¿ç”¨ç»Ÿä¸€åˆ†ç±»å™¨è¿›è¡Œæ‰¹é‡è¿‡æ»¤ (æ€§èƒ½ä¼˜åŒ–)
        if self.exclude_stablecoins or self.exclude_wrapped_coins:
            coin_ids = df["coin_id"].tolist()

            # æ‰¹é‡è¿‡æ»¤ï¼Œä¸€æ¬¡è°ƒç”¨å¤„ç†æ‰€æœ‰å¸ç§
            filtered_coin_ids = self.classifier.filter_coins(
                coin_ids=coin_ids,
                exclude_stablecoins=self.exclude_stablecoins,
                exclude_wrapped_coins=self.exclude_wrapped_coins,
                use_cache=True,
            )

            # åªä¿ç•™è¿‡æ»¤åçš„å¸ç§
            filtered_df = filtered_df[filtered_df["coin_id"].isin(filtered_coin_ids)]

            self.logger.debug(
                f"åˆ†ç±»è¿‡æ»¤: {len(df)} -> {len(filtered_df)} "
                f"(æ’é™¤ç¨³å®šå¸: {self.exclude_stablecoins}, æ’é™¤åŒ…è£…å¸: {self.exclude_wrapped_coins})"
            )

        return filtered_df

    def calculate_index(
        self,
        start_date: Union[str, date],
        end_date: Union[str, date],
        top_n: int = 510,
        base_date: Optional[Union[str, date]] = None,
        base_value: float = 1000.0,
    ) -> pd.DataFrame:
        """
        è®¡ç®—å¸‚å€¼åŠ æƒæŒ‡æ•° - ä¿®æ­£ç‰ˆ

        âœ… æ­£ç¡®ç®—æ³•ï¼šç›´æ¥æ¯”è¾ƒæ¯æ—¥å‰Nåæ€»å¸‚å€¼ï¼Œæ— éœ€ä»·æ ¼åŠ æƒè®¡ç®—
        å…¬å¼ï¼šIndex(t) = Base_Value Ã— [å½“æ—¥å‰Nåæ€»å¸‚å€¼ / åŸºå‡†æ—¥å‰Nåæ€»å¸‚å€¼]

        Args:
            start_date: æŒ‡æ•°è®¡ç®—å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: æŒ‡æ•°è®¡ç®—ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            base_date: åŸºå‡†æ—¥æœŸ (YYYY-MM-DD)
            base_value: åŸºå‡†æŒ‡æ•°å€¼
            top_n: æ¯æ—¥é€‰æ‹©çš„æˆåˆ†å¸ç§æ•°é‡

        Returns:
            åŒ…å«æŒ‡æ•°æ•°æ®çš„DataFrameï¼Œåˆ—ä¸ºï¼šdate, index_value, constituent_count
        """
        self.logger.info(f"å¼€å§‹è®¡ç®—å¸‚å€¼åŠ æƒæŒ‡æ•°: {start_date} åˆ° {end_date}")
        self.logger.info(f"åŸºå‡†æ—¥æœŸ: {base_date}, åŸºå‡†å€¼: {base_value}")
        self.logger.info(f"æ¯æ—¥æˆåˆ†å¸ç§æ•°: {top_n}")
        self.logger.info(
            f"æ’é™¤ç¨³å®šå¸: {self.exclude_stablecoins}, æ’é™¤åŒ…è£…å¸: {self.exclude_wrapped_coins}"
        )
        self.logger.info("âœ… ä½¿ç”¨ä¿®æ­£åçš„ç®—æ³•ï¼šç›´æ¥å¸‚å€¼æ¯”è¾ƒæ³•")
        if self.force_rebuild:
            self.logger.info(
                "å·²å¯ç”¨å¼ºåˆ¶é‡å»ºæ¯æ—¥æ•°æ®åŠŸèƒ½ï¼Œå°†ä½¿ç”¨æœ€æ–°çš„åŸå§‹æ•°æ®ç”Ÿæˆæ¯æ—¥æ±‡æ€»"
            )

        # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdateå¯¹è±¡
        if isinstance(start_date, str):
            start_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
        else:
            start_dt = start_date

        if isinstance(end_date, str):
            end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
        else:
            end_dt = end_date

        if base_date is None:
            base_dt = start_dt
        elif isinstance(base_date, str):
            base_dt = datetime.strptime(base_date, "%Y-%m-%d").date()
        else:
            base_dt = base_date

        # è·å–åŸºå‡†æ—¥æœŸçš„å¸‚å€¼æ•°æ®å’Œæˆåˆ†å¸ç§
        base_market_caps = self._get_daily_market_caps(base_dt)
        if not base_market_caps:
            raise ValueError(f"åŸºå‡†æ—¥æœŸ {base_date} æ²¡æœ‰å¯ç”¨çš„å¸‚å€¼æ•°æ®")

        base_constituents = self._select_top_coins(base_market_caps, top_n)
        actual_base_count = len(base_constituents)

        if actual_base_count < top_n:
            raise ValueError(
                f"åŸºå‡†æ—¥æœŸ {base_date} åªæœ‰ {actual_base_count} ä¸ªå¯ç”¨å¸ç§ï¼Œ"
                f"ä¸è¶³ä»¥æ»¡è¶³ top_n={top_n} çš„è¦æ±‚ã€‚"
            )

        # âœ… å…³é”®ä¿®æ­£ï¼šç›´æ¥è®¡ç®—åŸºå‡†æ—¥å‰Nåæ€»å¸‚å€¼
        base_total_market_cap = sum(
            base_market_caps[coin_id] for coin_id in base_constituents
        )

        self.logger.info(f"âœ… åŸºå‡†æ—¥æœŸ {base_dt} å‰{top_n}åå¸ç§")
        self.logger.info(f"å‰5å: {base_constituents[:5]}")
        self.logger.info(f"âœ… åŸºå‡†æ—¥æ€»å¸‚å€¼: ${base_total_market_cap:,.0f}")

        # ç”Ÿæˆæ—¥æœŸèŒƒå›´
        date_range = pd.date_range(start=start_dt, end=end_dt, freq="D")

        index_data = []

        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè®¡ç®—è¿›åº¦
        with tqdm(
            total=len(date_range),
            desc=f"è®¡ç®—æ­£ç¡®çš„å¸‚å€¼åŠ æƒæŒ‡æ•° (å‰{top_n}å)",
            unit="å¤©",
            ncols=100,
        ) as pbar:
            for i, current_dt in enumerate(date_range):
                current_date = current_dt.date()

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æ—¥æœŸ
                pbar.set_description(
                    f"è®¡ç®—æ­£ç¡®çš„å¸‚å€¼åŠ æƒæŒ‡æ•° - {current_date} ({i+1}/{len(date_range)})"
                )

                # è·å–å½“æ—¥å¸‚å€¼æ•°æ®å’Œæˆåˆ†å¸ç§
                current_market_caps = self._get_daily_market_caps(current_date)
                if not current_market_caps:
                    self.logger.warning(f"æ—¥æœŸ {current_date} æ²¡æœ‰å¯ç”¨çš„å¸‚å€¼æ•°æ®ï¼Œè·³è¿‡")
                    pbar.set_postfix_str(f"è·³è¿‡: {current_date}")
                    pbar.update(1)
                    continue

                current_constituents = self._select_top_coins(
                    current_market_caps, top_n
                )
                if not current_constituents:
                    self.logger.warning(f"æ—¥æœŸ {current_date} æ²¡æœ‰æ‰¾åˆ°æˆåˆ†å¸ç§ï¼Œè·³è¿‡")
                    pbar.set_postfix_str(f"è·³è¿‡: {current_date}")
                    pbar.update(1)
                    continue

                # âœ… å…³é”®ä¿®æ­£ï¼šç›´æ¥è®¡ç®—å½“æ—¥å‰Nåæ€»å¸‚å€¼ï¼Œæ— éœ€ä»·æ ¼è®¡ç®—
                current_total_market_cap = sum(
                    current_market_caps[coin_id] for coin_id in current_constituents
                )

                # âœ… æ­£ç¡®çš„æŒ‡æ•°è®¡ç®—å…¬å¼ï¼šç›´æ¥å¸‚å€¼æ¯”è¾ƒ
                index_value = base_value * (
                    current_total_market_cap / base_total_market_cap
                )

                index_data.append(
                    {
                        "date": current_date,
                        "index_value": index_value,
                        "constituent_count": len(current_constituents),
                    }
                )

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix_str(f"æŒ‡æ•°å€¼: {index_value:.2f}")
                pbar.update(1)

                if len(index_data) % 100 == 0:
                    self.logger.info(f"å·²å¤„ç† {len(index_data)} ä¸ªäº¤æ˜“æ—¥")

        if not index_data:
            raise ValueError(
                f"æ— æ³•ç”ŸæˆæŒ‡æ•°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´ {start_date} åˆ° {end_date} å†…æ²¡æœ‰å¯ç”¨æ•°æ®"
            )

        result_df = pd.DataFrame(index_data)

        self.logger.info(f"âœ… æŒ‡æ•°è®¡ç®—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(result_df)} ä¸ªæ•°æ®ç‚¹")
        self.logger.info(
            f"æŒ‡æ•°èŒƒå›´: {result_df['index_value'].min():.2f} - {result_df['index_value'].max():.2f}"
        )

        return result_df

    def save_index(self, index_df: pd.DataFrame, output_path: str) -> None:
        """
        ä¿å­˜æŒ‡æ•°æ•°æ®åˆ°CSVæ–‡ä»¶

        Args:
            index_df: æŒ‡æ•°æ•°æ®DataFrame
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        index_df.to_csv(output_file, index=False, float_format="%.6f")
        self.logger.info(f"æŒ‡æ•°æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
