#!/usr/bin/env python3
"""
å¢é‡æ¯æ—¥æ•°æ®æ›´æ–°å™¨

åŠŸèƒ½ï¼šæ£€æµ‹æ–°å¸ç§ã€ä¸‹è½½å†å²æ•°æ®ã€é›†æˆåˆ°æ¯æ—¥æ–‡ä»¶
ç‰¹æ€§ï¼šå¹¶è¡Œå¤„ç†ã€é”™è¯¯æ¢å¤ã€è‡ªåŠ¨æ’åº
"""

import json
import logging
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import date, datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import pandas as pd
from tqdm import tqdm

from ..api.coingecko import CoinGeckoAPI
from ..downloaders.batch_downloader import create_batch_downloader
from ..updaters.price_updater import MarketDataFetcher
from ..utils.path_utils import find_project_root, resolve_data_path, ensure_directory

logger = logging.getLogger(__name__)


class IncrementalDailyUpdater:
    """å¢é‡æ¯æ—¥æ•°æ®æ›´æ–°å™¨"""

    def __init__(
        self,
        coins_dir: str = "data/coins",
        daily_dir: str = "data/daily/daily_files",
        backup_enabled: bool = False,  # é»˜è®¤ç¦ç”¨å¤‡ä»½ï¼Œé¿å…äº§ç”Ÿå¤§é‡æ–‡ä»¶
        use_database: bool = True,  # ğŸš€ æ–°å¢ï¼šå¯ç”¨æ•°æ®åº“æ¨¡å¼ä»¥è·å¾—æ›´å¥½æ€§èƒ½
    ):
        """
        åˆå§‹åŒ–å¢é‡æ›´æ–°å™¨
        
        Args:
            coins_dir: å¸ç§æ•°æ®ç›®å½•
            daily_dir: æ¯æ—¥æ•°æ®ç›®å½•
            backup_enabled: æ˜¯å¦å¯ç”¨å¤‡ä»½åŠŸèƒ½
            use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆæ¨èå¼€å¯ä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰
        """
        # ä½¿ç”¨æ–°çš„è·¯å¾„å·¥å…·
        self.project_root = find_project_root()
        
        # è§£æè·¯å¾„ï¼šä½¿ç”¨æ–°çš„è·¯å¾„å·¥å…·
        self.coins_dir = resolve_data_path(coins_dir, self.project_root)
        self.daily_dir = resolve_data_path(daily_dir, self.project_root)
        self.backup_enabled = backup_enabled
        self.use_database = use_database

        # åˆå§‹åŒ–ä¾èµ–ç»„ä»¶
        self.downloader = create_batch_downloader()
        api = CoinGeckoAPI()
        self.market_fetcher = MarketDataFetcher(api)

        # ğŸš€ åˆå§‹åŒ–æ•°æ®åº“æ”¯æŒçš„æ•°æ®èšåˆå™¨
        if use_database:
            from ..downloaders.daily_aggregator import DailyDataAggregator
            self.daily_aggregator = DailyDataAggregator(
                data_dir=str(self.coins_dir.parent / "coins"),
                output_dir=str(self.daily_dir.parent),
                use_database=True
            )

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        ensure_directory(self.daily_dir)

        # æ“ä½œæ—¥å¿—æ–‡ä»¶
        operation_log_path = "logs/incremental_daily_operations.jsonl"
        self.operation_log = resolve_data_path(operation_log_path, self.project_root)
        ensure_directory(self.operation_log.parent)

        logger.info("å¢é‡æ›´æ–°å™¨åˆå§‹åŒ–å®Œæˆ")

    def get_existing_coins(self) -> Set[str]:
        """è·å–å·²æœ‰çš„å¸ç§åˆ—è¡¨"""
        existing = set()
        for csv_file in self.coins_dir.glob("*.csv"):
            existing.add(csv_file.stem)
        logger.debug(f"å‘ç° {len(existing)} ä¸ªå·²æœ‰å¸ç§")
        return existing

    def get_current_market_coins(self, top_n: int = 1000) -> Set[str]:
        """è·å–å½“å‰å¸‚å€¼å‰Nåå¸ç§"""
        logger.info(f"è·å–å½“å‰å¸‚å€¼å‰ {top_n} åå¸ç§...")
        try:
            market_data = self.market_fetcher.get_top_coins(top_n)
            coin_ids = {coin["id"] for coin in market_data}
            logger.info(f"æˆåŠŸè·å– {len(coin_ids)} ä¸ªå¸‚å€¼æ’åå¸ç§")
            return coin_ids
        except Exception as e:
            logger.error(f"è·å–å¸‚å€¼æ’åå¤±è´¥: {e}")
            return set()

    def detect_new_coins(self, top_n: int = 1000) -> List[str]:
        """æ£€æµ‹æ–°å¸ç§

        Returns:
            æ–°å¸ç§IDåˆ—è¡¨
        """
        logger.info("å¼€å§‹æ£€æµ‹æ–°å¸ç§...")

        existing = self.get_existing_coins()
        current = self.get_current_market_coins(top_n)

        if not current:
            logger.warning("æ— æ³•è·å–å½“å‰å¸‚å€¼æ’åï¼Œè·³è¿‡æ–°å¸ç§æ£€æµ‹")
            return []

        new_coins = current - existing

        if new_coins:
            logger.info(f"å‘ç° {len(new_coins)} ä¸ªæ–°å¸ç§")
            for coin in sorted(new_coins):
                logger.info(f"  - {coin}")
        else:
            logger.info("æ²¡æœ‰å‘ç°æ–°å¸ç§")

        return list(new_coins)

    def download_new_coin_history(self, coin_id: str) -> bool:
        """ä¸‹è½½æ–°å¸ç§çš„å®Œæ•´å†å²æ•°æ®

        Args:
            coin_id: å¸ç§ID

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        logger.info(f"å¼€å§‹ä¸‹è½½ {coin_id} çš„å†å²æ•°æ®")

        try:
            # ä½¿ç”¨ max days è·å–å®Œæ•´å†å²
            success = self.downloader.download_coin_data(coin_id, days="max")

            if success:
                logger.info(f"{coin_id} å†å²æ•°æ®ä¸‹è½½æˆåŠŸ")
                # è®°å½•æ“ä½œæ—¥å¿—
                self._log_operation("download", coin_id, success=True)
            else:
                logger.error(f"{coin_id} å†å²æ•°æ®ä¸‹è½½å¤±è´¥")
                self._log_operation(
                    "download", coin_id, success=False, error="ä¸‹è½½å¤±è´¥"
                )

            return success

        except Exception as e:
            error_msg = f"ä¸‹è½½ {coin_id} å†å²æ•°æ®æ—¶å‡ºé”™: {e}"
            logger.error(error_msg)
            self._log_operation("download", coin_id, success=False, error=str(e))
            return False

    def load_coin_data(self, coin_id: str) -> Optional[pd.DataFrame]:
        """åŠ è½½å¸ç§æ•°æ®

        Args:
            coin_id: å¸ç§ID

        Returns:
            å¸ç§æ•°æ®DataFrameï¼Œå¤±è´¥åˆ™è¿”å›None
        """
        csv_path = self.coins_dir / f"{coin_id}.csv"
        if not csv_path.exists():
            logger.debug(f"å¸ç§æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return None

        try:
            df = pd.read_csv(csv_path)

            # æ•°æ®éªŒè¯
            required_columns = ["timestamp", "price", "market_cap"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.error(f"{coin_id} æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return None

            # è½¬æ¢æ—¥æœŸ
            df["date"] = pd.to_datetime(df["timestamp"], unit="ms").dt.date
            df["coin_id"] = coin_id

            # æ•°æ®æ¸…æ´—ï¼šç§»é™¤æ— æ•ˆè®°å½•
            original_len = len(df)
            df = df.dropna(subset=["price", "market_cap"])
            df = df[(df["price"] > 0) & (df["market_cap"] > 0)]

            if len(df) < original_len:
                logger.warning(f"{coin_id} æ¸…ç†äº† {original_len - len(df)} æ¡æ— æ•ˆè®°å½•")

            logger.debug(f"æˆåŠŸåŠ è½½ {coin_id} æ•°æ®: {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"åŠ è½½ {coin_id} æ•°æ®å¤±è´¥: {e}")
            return None

    def get_existing_daily_dates(self) -> Set[date]:
        """è·å–å·²æœ‰çš„æ¯æ—¥æ•°æ®æ–‡ä»¶æ—¥æœŸ"""
        dates = set()

        # æ‰«æåˆ†å±‚ç»“æ„: YYYY/MM/YYYY-MM-DD.csv
        for year_dir in self.daily_dir.iterdir():
            if year_dir.is_dir() and year_dir.name.isdigit():
                for month_dir in year_dir.iterdir():
                    if month_dir.is_dir() and month_dir.name.isdigit():
                        for csv_file in month_dir.glob("*.csv"):
                            try:
                                file_date = datetime.strptime(
                                    csv_file.stem, "%Y-%m-%d"
                                ).date()
                                dates.add(file_date)
                            except ValueError:
                                continue

        logger.debug(f"å‘ç° {len(dates)} ä¸ªå·²æœ‰æ¯æ—¥æ•°æ®æ–‡ä»¶")
        return dates

    def _backup_daily_file(self, filepath: Path) -> Optional[Path]:
        """å¤‡ä»½æ¯æ—¥æ•°æ®æ–‡ä»¶ï¼ˆæ™ºèƒ½å¤‡ä»½ï¼‰

        Args:
            filepath: è¦å¤‡ä»½çš„æ–‡ä»¶è·¯å¾„

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥åˆ™è¿”å›None
        """
        if not self.backup_enabled or not filepath.exists():
            return None

        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = filepath.parent / ".backup"
            backup_dir.mkdir(exist_ok=True)

            # æ™ºèƒ½å¤‡ä»½ï¼šåªä¿ç•™æœ€æ–°çš„3ä¸ªå¤‡ä»½
            existing_backups = sorted(
                backup_dir.glob(f"{filepath.stem}_*.csv"),
                key=lambda x: x.stat().st_mtime,
                reverse=True,
            )

            # åˆ é™¤è¶…è¿‡3ä¸ªçš„æ—§å¤‡ä»½
            for old_backup in existing_backups[2:]:  # ä¿ç•™æœ€æ–°2ä¸ªï¼Œç¬¬3ä¸ªå¼€å§‹åˆ é™¤
                try:
                    old_backup.unlink()
                    logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ—§å¤‡ä»½å¤±è´¥: {e}")

            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"{filepath.stem}_{timestamp}.csv"
            backup_path = backup_dir / backup_filename

            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(filepath, backup_path)
            logger.debug(f"å·²å¤‡ä»½æ–‡ä»¶: {backup_path}")
            return backup_path

        except Exception as e:
            logger.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None

    def insert_coin_into_daily_file(
        self, target_date: date, coin_data: Dict, pbar: Optional[tqdm] = None
    ) -> bool:
        """å°†å¸ç§æ•°æ®æ’å…¥åˆ°æŒ‡å®šæ—¥æœŸçš„æ¯æ—¥æ–‡ä»¶ä¸­

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ
            coin_data: å¸ç§æ•°æ®å­—å…¸
            pbar: tqdm è¿›åº¦æ¡å®ä¾‹ (å¯é€‰)

        Returns:
            æ˜¯å¦æ’å…¥æˆåŠŸ
        """
        try:
            # æ„é€ æ–‡ä»¶è·¯å¾„
            year_dir = self.daily_dir / str(target_date.year)
            month_dir = year_dir / f"{target_date.month:02d}"
            filepath = month_dir / f"{target_date}.csv"

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            month_dir.mkdir(parents=True, exist_ok=True)

            # å¤‡ä»½ç°æœ‰æ–‡ä»¶
            backup_path = self._backup_daily_file(filepath)

            try:
                if filepath.exists():
                    # è¯»å–ç°æœ‰æ–‡ä»¶
                    df = pd.read_csv(filepath)

                    # æ£€æŸ¥å¸ç§æ˜¯å¦å·²å­˜åœ¨
                    if coin_data["coin_id"] in df["coin_id"].values:
                        logger.debug(
                            f"{coin_data['coin_id']} åœ¨ {target_date} å·²å­˜åœ¨ï¼Œè·³è¿‡"
                        )
                        return True

                    # æ·»åŠ æ–°å¸ç§æ•°æ®
                    new_row = pd.DataFrame([coin_data])
                    df = pd.concat([df, new_row], ignore_index=True)

                else:
                    # åˆ›å»ºæ–°æ–‡ä»¶
                    df = pd.DataFrame([coin_data])

                # é‡æ–°æ’åºå¹¶æ›´æ–°æ’å
                df = df.sort_values("market_cap", ascending=False).reset_index(
                    drop=True
                )
                df["rank"] = range(1, len(df) + 1)

                # ä¿å­˜æ–‡ä»¶
                df.to_csv(filepath, index=False, float_format="%.6f")

                # æ›´æ–°æ—¥å¿—ï¼Œä½†ä¸ä½¿ç”¨ f-string ä»¥æé«˜æ€§èƒ½
                if pbar:
                    pbar.set_description(
                        f"å·²é›†æˆ {coin_data['coin_id']} åˆ° {target_date}"
                    )
                else:
                    logger.info(
                        f"å·²å°† {coin_data['coin_id']} æ’å…¥åˆ° {target_date} (æ’å: {df[df['coin_id'] == coin_data['coin_id']]['rank'].iloc[0]})"
                    )

                # è®°å½•æ“ä½œæ—¥å¿—
                self._log_operation(
                    "insert",
                    coin_data["coin_id"],
                    success=True,
                    target_date=str(target_date),
                    rank=int(df[df["coin_id"] == coin_data["coin_id"]]["rank"].iloc[0]),
                )

                return True

            except Exception as e:
                # å¦‚æœæœ‰å¤‡ä»½ï¼Œå°è¯•æ¢å¤
                if backup_path and backup_path.exists():
                    try:
                        shutil.copy2(backup_path, filepath)
                        logger.warning(f"æ“ä½œå¤±è´¥ï¼Œå·²ä»å¤‡ä»½æ¢å¤: {filepath}")
                    except Exception as restore_error:
                        logger.error(f"æ¢å¤å¤‡ä»½å¤±è´¥: {restore_error}")

                raise e

        except Exception as e:
            error_msg = f"æ’å…¥ {coin_data['coin_id']} åˆ° {target_date} å¤±è´¥: {e}"
            logger.error(error_msg)
            self._log_operation(
                "insert",
                coin_data["coin_id"],
                success=False,
                error=str(e),
                target_date=str(target_date),
            )
            return False

    def integrate_new_coin_into_daily_files(self, coin_id: str) -> Tuple[int, int]:
        """å°†æ–°å¸ç§æ•°æ®é›†æˆåˆ°æ‰€æœ‰ç›¸å…³çš„æ¯æ—¥æ–‡ä»¶ä¸­

        Args:
            coin_id: å¸ç§ID

        Returns:
            (æˆåŠŸæ’å…¥å¤©æ•°, æ€»å°è¯•å¤©æ•°)
        """
        # logger.info(f"å¼€å§‹é›†æˆ {coin_id} åˆ°æ¯æ—¥æ–‡ä»¶") # åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹è¿‡äºå˜ˆæ‚

        # åŠ è½½å¸ç§æ•°æ®
        coin_df = self.load_coin_data(coin_id)
        if coin_df is None:
            logger.error(f"æ— æ³•åŠ è½½ {coin_id} æ•°æ®")
            return 0, 0

        # è·å–å·²æœ‰çš„æ¯æ—¥æ–‡ä»¶æ—¥æœŸ
        existing_dates = self.get_existing_daily_dates()

        # æ‰¾åˆ°å¸ç§æ•°æ®ä¸å·²æœ‰æ—¥æœŸçš„äº¤é›†
        coin_dates = set(coin_df["date"].unique())
        relevant_dates = existing_dates.intersection(coin_dates)

        total_attempts = len(relevant_dates)
        if total_attempts == 0:
            # logger.warning(f"{coin_id} æ•°æ®ä¸ç°æœ‰æ¯æ—¥æ–‡ä»¶æ— äº¤é›†") # åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹è¿‡äºå˜ˆæ‚
            return 0, 0

        # logger.info(
        #     f"{coin_id} æœ‰ {len(coin_dates)} å¤©æ•°æ®ï¼Œå…¶ä¸­ {total_attempts} å¤©ä¸å·²æœ‰æ–‡ä»¶é‡å "
        # )

        # é€æ—¥æ’å…¥
        successful_insertions = 0

        for target_date in sorted(relevant_dates):
            # è·å–è¯¥æ—¥æœŸçš„å¸ç§æ•°æ®
            day_data = coin_df[coin_df["date"] == target_date]
            if day_data.empty:
                continue

            # å–æœ€æ–°è®°å½•ï¼ˆé˜²æ­¢åŒæ—¥å¤šæ¡è®°å½•ï¼‰
            latest_record = day_data.iloc[-1]

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if (
                pd.isna(latest_record["price"])
                or latest_record["price"] <= 0
                or pd.isna(latest_record["market_cap"])
                or latest_record["market_cap"] <= 0
            ):
                logger.debug(f"{coin_id} åœ¨ {target_date} çš„æ•°æ®æ— æ•ˆï¼Œè·³è¿‡")
                continue

            # æ„é€ æ•°æ®è®°å½•
            coin_data = {
                "timestamp": int(latest_record["timestamp"]),
                "price": float(latest_record["price"]),
                "volume": (
                    float(latest_record["volume"])
                    if pd.notna(latest_record["volume"])
                    else 0.0
                ),
                "market_cap": float(latest_record["market_cap"]),
                "date": target_date,
                "coin_id": coin_id,
            }

            # æ’å…¥åˆ°æ¯æ—¥æ–‡ä»¶
            if self.insert_coin_into_daily_file(target_date, coin_data):
                successful_insertions += 1

        success_rate = (
            (successful_insertions / total_attempts * 100) if total_attempts > 0 else 0
        )
        logger.info(
            f"{coin_id} é›†æˆå®Œæˆ: {successful_insertions}/{total_attempts} å¤©æˆåŠŸ ({success_rate:.1f}%)"
        )

        return successful_insertions, total_attempts

    def _log_operation(self, operation: str, coin_id: str, success: bool, **kwargs):
        """è®°å½•æ“ä½œæ—¥å¿—

        Args:
            operation: æ“ä½œç±»å‹ (download, insert, etc.)
            coin_id: å¸ç§ID
            success: æ˜¯å¦æˆåŠŸ
            **kwargs: å…¶ä»–ä¿¡æ¯
        """
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "operation": operation,
                "coin_id": coin_id,
                "success": success,
                **kwargs,
            }

            with open(self.operation_log, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry) + "\n")

        except Exception as e:
            logger.warning(f"è®°å½•æ“ä½œæ—¥å¿—å¤±è´¥: {e}")

    def update_with_new_coins(
        self, top_n: int = 1000, max_workers: int = 3, dry_run: bool = False
    ) -> Dict:
        """æ£€æµ‹å¹¶é›†æˆæ–°å¸ç§çš„å®Œæ•´æµç¨‹

        Args:
            top_n: ç›‘æ§çš„å¸‚å€¼æ’åèŒƒå›´
            max_workers: å¹¶è¡Œä¸‹è½½çš„å·¥ä½œçº¿ç¨‹æ•°
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰

        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        start_time = datetime.now()
        logger.info("=" * 60)
        logger.info("å¼€å§‹å¢é‡æ¯æ—¥æ•°æ®æ›´æ–°")
        logger.info(f"ç›‘æ§èŒƒå›´: å‰ {top_n} å")
        logger.info(f"å¹¶è¡Œçº¿ç¨‹: {max_workers}")
        logger.info(f"è¯•è¿è¡Œæ¨¡å¼: {'æ˜¯' if dry_run else 'å¦'}")
        logger.info("=" * 60)

        results = {
            "summary": {
                "start_time": start_time.isoformat(),
                "top_n": top_n,
                "dry_run": dry_run,
            },
            "new_coins": [],
            "download_results": {},
            "integration_results": {},
        }

        try:
            # 1. æ£€æµ‹æ–°å¸ç§
            new_coins = self.detect_new_coins(top_n)
            results["new_coins"] = new_coins

            if not new_coins:
                logger.info("æ²¡æœ‰å‘ç°æ–°å¸ç§ï¼Œæ— éœ€æ›´æ–°")
                results["summary"]["status"] = "no_new_coins"
                return results

            if dry_run:
                logger.info(
                    f"è¯•è¿è¡Œæ¨¡å¼ï¼šå‘ç° {len(new_coins)} ä¸ªæ–°å¸ç§ï¼Œå®é™…è¿è¡Œæ—¶å°†ä¼šä¸‹è½½å¹¶é›†æˆ"
                )
                results["summary"]["status"] = "dry_run_complete"
                return results

            # 2. ä¸‹è½½æ–°å¸ç§å†å²æ•°æ®
            logger.info(f"å¼€å§‹ä¸‹è½½ {len(new_coins)} ä¸ªæ–°å¸ç§çš„å†å²æ•°æ®")

            with tqdm(total=len(new_coins), desc="ä¸‹è½½æ–°å¸ç§æ•°æ®") as pbar:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_coin = {
                        executor.submit(self.download_new_coin_history, coin): coin
                        for coin in new_coins
                    }

                    for future in as_completed(future_to_coin):
                        coin = future_to_coin[future]
                        pbar.set_description(f"ä¸‹è½½ {coin}")
                        try:
                            success = future.result()
                            results["download_results"][coin] = {
                                "success": success,
                                "error": None,
                            }
                        except Exception as e:
                            logger.error(f"ä¸‹è½½ {coin} æ—¶å‡ºé”™: {e}")
                            results["download_results"][coin] = {
                                "success": False,
                                "error": str(e),
                            }
                        pbar.update(1)

            # 3. é›†æˆåˆ°æ¯æ—¥æ–‡ä»¶ (å¹¶è¡ŒåŒ–)
            logger.info("å¼€å§‹å¹¶è¡Œé›†æˆæ–°å¸ç§æ•°æ®åˆ°æ¯æ—¥æ–‡ä»¶")
            successful_coins = [
                coin
                for coin, res in results["download_results"].items()
                if res["success"]
            ]

            with tqdm(total=len(successful_coins), desc="é›†æˆæ–°å¸ç§") as pbar:
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    future_to_coin = {
                        executor.submit(
                            self.integrate_new_coin_into_daily_files, coin
                        ): coin
                        for coin in successful_coins
                    }

                    for future in as_completed(future_to_coin):
                        coin = future_to_coin[future]
                        pbar.set_description(f"é›†æˆ {coin}")
                        try:
                            inserted_count, total_attempts = future.result()
                            results["integration_results"][coin] = {
                                "success": inserted_count > 0,
                                "inserted_days": inserted_count,
                                "total_attempts": total_attempts,
                                "success_rate": (
                                    (inserted_count / total_attempts * 100)
                                    if total_attempts > 0
                                    else 0
                                ),
                                "error": None,
                            }
                        except Exception as e:
                            logger.error(f"é›†æˆ {coin} æ—¶å‡ºé”™: {e}")
                            results["integration_results"][coin] = {
                                "success": False,
                                "inserted_days": 0,
                                "total_attempts": 0,
                                "success_rate": 0,
                                "error": str(e),
                            }
                        pbar.update(1)

            # æ ‡è®°ä¸‹è½½å¤±è´¥çš„å¸ç§
            for coin in new_coins:
                if coin not in successful_coins:
                    results["integration_results"][coin] = {
                        "success": False,
                        "inserted_days": 0,
                        "total_attempts": 0,
                        "success_rate": 0,
                        "error": "ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡é›†æˆ",
                    }

            # 4. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            successful_downloads = sum(
                1 for r in results["download_results"].values() if r["success"]
            )
            successful_integrations = sum(
                1 for r in results["integration_results"].values() if r["success"]
            )
            total_insertions = sum(
                r.get("inserted_days", 0)
                for r in results["integration_results"].values()
            )

            results["summary"].update(
                {
                    "end_time": end_time.isoformat(),
                    "duration_seconds": duration,
                    "successful_downloads": successful_downloads,
                    "successful_integrations": successful_integrations,
                    "total_insertions": total_insertions,
                    "status": "completed",
                }
            )

            logger.info(
                f"å¢é‡æ›´æ–°å®Œæˆï¼š{successful_downloads} ä¸ªæ–°å¸ç§å†å²æ•°æ®ä¸‹è½½æˆåŠŸï¼Œ{successful_integrations} ä¸ªå¸ç§æ•°æ®æˆåŠŸé›†æˆ"
            )
        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­äº†å¢é‡æ›´æ–°æ“ä½œ")
            results["summary"]["status"] = "interrupted"
            results["summary"]["error"] = "User interrupted the process"
            return results
        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            results["summary"]["status"] = "error"
            results["summary"]["error"] = str(e)
            return results

        # è‡ªåŠ¨é‡æ’åº
        try:
            logger.info("å°è¯•å¯¹æ¯æ—¥æ–‡ä»¶è¿›è¡Œè‡ªåŠ¨é‡æ’åº...")
            # è·å–æ‰€æœ‰æ¯æ—¥æ–‡ä»¶
            daily_files = [
                f for f in self.daily_dir.glob("*/*/*.csv") if f.stem != f.parent.name
            ]

            if not daily_files:
                logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡æ’åºçš„æ¯æ—¥æ–‡ä»¶ã€‚")
            else:
                logger.info(f"æ‰¾åˆ° {len(daily_files)} ä¸ªæ–‡ä»¶éœ€è¦æ£€æŸ¥å’Œé‡æ’åºã€‚")

            for target_file in tqdm(daily_files, desc="é‡æ’åºæ¯æ—¥æ–‡ä»¶"):
                try:
                    # å¤‡ä»½ç°æœ‰æ–‡ä»¶
                    self._backup_daily_file(target_file)

                    # æ‰§è¡Œæ’åº
                    df = pd.read_csv(target_file)
                    df = df.sort_values("market_cap", ascending=False).reset_index(
                        drop=True
                    )
                    df["rank"] = range(1, len(df) + 1)
                    df.to_csv(target_file, index=False, float_format="%.6f")

                    logger.debug(f"å·²å¯¹ {target_file} è¿›è¡Œé‡æ’åº")
                except Exception as sort_e:
                    logger.error(f"é‡æ’åºæ–‡ä»¶ {target_file} å¤±è´¥: {sort_e}")
                    # è®°å½•åˆ°ç»“æœä¸­ï¼Œä½†ä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    if "resorting_errors" not in results["summary"]:
                        results["summary"]["resorting_errors"] = []
                    results["summary"]["resorting_errors"].append(
                        f"File: {target_file}, Error: {str(sort_e)}"
                    )

            logger.info("è‡ªåŠ¨é‡æ’åºå®Œæˆã€‚")

        except Exception as e:
            logger.error(f"è‡ªåŠ¨é‡æ’åºè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            results["summary"]["status"] = "completed_with_resorting_error"
            results["summary"]["resorting_error"] = str(e)

        return results


def create_incremental_updater(
    coins_dir: str = "data/coins",
    daily_dir: str = "data/daily/daily_files",
    backup_enabled: bool = False,  # é»˜è®¤ç¦ç”¨å¤‡ä»½
    use_database: bool = True,  # ğŸš€ æ–°å¢ï¼šé»˜è®¤å¯ç”¨æ•°æ®åº“æ¨¡å¼
) -> IncrementalDailyUpdater:
    """åˆ›å»ºå¢é‡æ¯æ—¥æ•°æ®æ›´æ–°å™¨å®ä¾‹

    Args:
        coins_dir: å¸ç§æ•°æ®ç›®å½•
        daily_dir: æ¯æ—¥æ•°æ®ç›®å½•
        backup_enabled: æ˜¯å¦å¯ç”¨å¤‡ä»½
        use_database: æ˜¯å¦å¯ç”¨æ•°æ®åº“æ¨¡å¼ï¼ˆæ¨èå¼€å¯ä»¥è·å¾—æ›´å¥½æ€§èƒ½ï¼‰

    Returns:
        IncrementalDailyUpdater å®ä¾‹
    """
    return IncrementalDailyUpdater(coins_dir, daily_dir, backup_enabled, use_database)
