"""
ä»·æ ¼æ•°æ®æ›´æ–°å™¨

æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ï¼Œæä¾›æ™ºèƒ½çš„ä»·æ ¼æ•°æ®æ›´æ–°ç­–ç•¥ã€‚

è®¾è®¡å“²å­¦ï¼š
1. ç®€å•èƒœäºå¤æ‚ - æ¯ä¸ªæ–¹æ³•èŒè´£å•ä¸€
2. ä¿¡ä»»æƒå¨æ•°æ®æº - åŸºäºCoinGeckoå®˜æ–¹åˆ†ç±»
3. ç”¨æˆ·å¯¼å‘è®¾è®¡ - ç¡®ä¿ç”¨æˆ·éœ€æ±‚å¾—åˆ°æ»¡è¶³

æ ¸å¿ƒç­–ç•¥ï¼š
1. æŒ‰å¸‚å€¼é¡ºåºè·å–å¸ç§
2. åŸºäºCoinGecko categoryç®€å•åˆ†ç±»ï¼šéåŒ…è£…å¸ä¸”éç¨³å®šå¸ = åŸç”Ÿå¸
3. æŒ‰é¡ºåºæ›´æ–°ï¼Œç¡®ä¿åŸç”Ÿå¸è¾¾åˆ°ç›®æ ‡æ•°é‡
4. åŒæ—¶æ›´æ–°é‡åˆ°çš„éåŸç”Ÿå¸ï¼ˆä¸ºæœªæ¥ç ”ç©¶å‡†å¤‡ï¼‰

âš ï¸ é‡è¦æé†’ï¼š
- æœ¬æ›´æ–°å™¨ä½¿ç”¨è¦†ç›–æ¨¡å¼ï¼Œä¼šå®Œå…¨é‡æ–°ç”ŸæˆCSVæ–‡ä»¶
- è‡ªåŠ¨è§£å†³å®æ—¶æ•°æ®æ—¶é—´æˆ³å¼‚å¸¸é—®é¢˜ï¼ˆå¦‚07:08:54æ ¼å¼çš„éæ ‡å‡†æ—¶é—´æˆ³ï¼‰
- è¯¦è§ï¼šdocs/timestamp_handling_memo.md
"""

import logging
import math
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

from tqdm import tqdm

from ..api.coingecko import CoinGeckoAPI
from ..classification.unified_classifier import UnifiedClassifier
from ..downloaders.batch_downloader import create_batch_downloader

# APIé™æµé…ç½®
RATE_LIMIT_CONFIG = {
    "delay_seconds": 0.13,
    "calls_per_minute": 461.5,
}

logger = logging.getLogger(__name__)


class MarketDataFetcher:
    """å¸‚åœºæ•°æ®è·å–å™¨ - èŒè´£å•ä¸€ï¼šè·å–å¸‚å€¼æ’åæ•°æ®"""

    def __init__(self, api: CoinGeckoAPI):
        self.api = api

    def get_top_coins(self, n: int) -> List[Dict]:
        """
        è·å–å¸‚å€¼å‰Nåå¸ç§

        Args:
            n: ç›®æ ‡å¸ç§æ•°é‡

        Returns:
            å¸ç§åˆ—è¡¨ï¼ŒæŒ‰å¸‚å€¼æ’åº
        """
        logger.info(f"ğŸ” è·å–å¸‚å€¼å‰ {n} ååŠ å¯†è´§å¸")

        coins = []
        pages = math.ceil(n / 250)  # æ¯é¡µæœ€å¤š250ä¸ª

        with tqdm(
            total=pages,
            desc="è·å–å¸‚å€¼æ’å",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]",
            leave=False,
        ) as pbar:
            for page in range(1, pages + 1):
                try:
                    # è®¡ç®—è¿™ä¸€é¡µåº”è¯¥è·å–å¤šå°‘ä¸ªå¸ç§
                    per_page = min(250, n - len(coins))
                    market_data = self.api.get_coins_markets(
                        vs_currency="usd",
                        order="market_cap_desc",
                        per_page=per_page,
                        page=page,
                        sparkline=False,
                    )

                    if not market_data:
                        logger.warning(f"ç¬¬ {page} é¡µæœªè·å–åˆ°æ•°æ®ï¼Œåœæ­¢è·å–")
                        break

                    for coin in market_data:
                        if len(coins) >= n:
                            break
                        coins.append(
                            {
                                "id": coin["id"],
                                "symbol": coin["symbol"],
                                "name": coin["name"],
                                "market_cap_rank": coin.get("market_cap_rank", 0),
                            }
                        )

                    pbar.set_postfix({"å·²è·å–": len(coins), "ç›®æ ‡": n, "å½“å‰é¡µ": page})
                    pbar.update(1)

                    if len(coins) >= n:
                        break

                except Exception as e:
                    logger.error(f"è·å–ç¬¬ {page} é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
                    break

                # APIé™æµå»¶è¿Ÿ
                time.sleep(RATE_LIMIT_CONFIG["delay_seconds"])

        logger.info(f"âœ… æˆåŠŸè·å– {len(coins)} ä¸ªå¸ç§çš„å¸‚å€¼æ’å")
        return coins[:n]


class PriceDataUpdater:
    """ä»·æ ¼æ•°æ®æ›´æ–°å™¨ - ä¸»è¦é€»è¾‘åè°ƒè€…"""

    def __init__(self):
        self.api = CoinGeckoAPI()
        self.downloader = create_batch_downloader()
        self.classifier = UnifiedClassifier()  # ç›´æ¥ä½¿ç”¨ç»Ÿä¸€åˆ†ç±»å™¨
        self.market_fetcher = MarketDataFetcher(self.api)

        # ç›®å½•è®¾ç½®
        self.coins_dir = Path("data/coins")
        self.metadata_dir = Path("data/metadata")

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_processed": 0,
            "native_updated": 0,
            "stable_updated": 0,
            "wrapped_updated": 0,
            "failed_updates": 0,
            "new_coins": 0,
            "api_calls": 0,
        }

        self.errors = []

    def download_coin_data(self, coin_id: str) -> Tuple[bool, bool]:
        """
        ä¸‹è½½å¸ç§æ•°æ®

        é‡è¦è®¾è®¡åŸåˆ™ï¼š
        å¯¹äº CoinGecko APIï¼Œå…¨é‡æ›´æ–°å’Œå¢é‡æ›´æ–°çš„ API æƒé‡æ¶ˆè€—ä¸€è‡´ï¼Œ
        å› æ­¤å§‹ç»ˆä½¿ç”¨å…¨é‡æ›´æ–° (days="max") æ¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼Œ
        é¿å…å¢é‡æ›´æ–°å¯èƒ½å¯¼è‡´çš„å†å²æ•°æ®ä¸¢å¤±é—®é¢˜ã€‚

        æ™ºèƒ½è·³è¿‡ç­–ç•¥ï¼š
        1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¿®æ”¹æ—¶é—´æ˜¯ä»Šå¤©
        2. æ£€æŸ¥æ•°æ®è¡Œæ•°æ˜¯å¦å……è¶³ï¼ˆ>500è¡Œï¼‰
        3. æ£€æŸ¥æ˜¯å¦æœ‰ä»Šæ—¥çš„æ•°æ®

        Returns:
            Tuple[bool, bool]: (success, api_called)
            - success: æ˜¯å¦æˆåŠŸï¼ˆåŒ…æ‹¬è·³è¿‡çš„æƒ…å†µï¼‰
            - api_called: æ˜¯å¦å®é™…è°ƒç”¨äº†API
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆä½¿ç”¨æ”¹è¿›çš„æ•°æ®è´¨é‡æ£€æŸ¥ï¼‰
            csv_file = self.coins_dir / f"{coin_id}.csv"
            if csv_file.exists():
                if self._check_data_quality(csv_file):
                    logger.info(f"â­ï¸ {coin_id} æ•°æ®è´¨é‡è‰¯å¥½ï¼Œè·³è¿‡ä¸‹è½½")
                    return True, False  # æˆåŠŸä½†æ²¡æœ‰APIè°ƒç”¨
                else:
                    logger.info(f"âš ï¸ {coin_id} æ•°æ®è´¨é‡éœ€è¦æ”¹å–„ï¼Œé‡æ–°ä¸‹è½½")

            # ç»Ÿä¸€ä½¿ç”¨å…¨é‡æ›´æ–°ç­–ç•¥
            logger.info(f"ğŸ“¥ ä¸‹è½½ {coin_id} å®Œæ•´å†å²æ•°æ® (å…¨é‡æ›´æ–°)...")
            success = self.downloader.download_coin_data(coin_id, days="max")

            if success:
                logger.info(f"âœ… {coin_id} æ•°æ®ä¸‹è½½æˆåŠŸ")
                return True, True  # æˆåŠŸä¸”æœ‰APIè°ƒç”¨
            else:
                logger.error(f"âŒ {coin_id} æ•°æ®ä¸‹è½½å¤±è´¥")
                return False, True  # å¤±è´¥ä½†æœ‰APIè°ƒç”¨

        except Exception as e:
            logger.error(f"ä¸‹è½½ {coin_id} æ•°æ®æ—¶å‡ºé”™: {e}")
            return False, True  # å¤±è´¥ä½†æœ‰APIè°ƒç”¨

    def _check_data_quality(self, csv_file: Path) -> bool:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡

        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ•°æ®è´¨é‡æ˜¯å¦è‰¯å¥½
        """
        try:
            import os
            from datetime import date

            import pandas as pd

            # 1. æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            mtime = os.path.getmtime(csv_file)
            file_date = date.fromtimestamp(mtime)
            today = date.today()

            # å¦‚æœä¸æ˜¯ä»Šå¤©ä¿®æ”¹çš„ï¼Œéœ€è¦æ›´æ–°
            if file_date != today:
                return False

            # 2. æ£€æŸ¥æ•°æ®å†…å®¹
            try:
                df = pd.read_csv(csv_file)
            except Exception:
                return False  # è¯»å–å¤±è´¥ï¼Œéœ€è¦é‡æ–°ä¸‹è½½

            # 3. æ£€æŸ¥æ•°æ®è¡Œæ•°ï¼ˆè‡³å°‘500è¡Œï¼‰
            if len(df) < 500:
                return False

            # 4. æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åˆ—
            if "timestamp" not in df.columns:
                return False

            # 5. æ£€æŸ¥æœ€æ–°æ•°æ®æ—¥æœŸ
            try:
                # è½¬æ¢timestampï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ä¸ºæ—¥æœŸ
                df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
                latest_date = df["timestamp"].dt.date.max()

                # æœ€æ–°æ•°æ®åº”è¯¥æ˜¯ä»Šå¤©æˆ–æ˜¨å¤©ï¼ˆè€ƒè™‘æ—¶åŒºå·®å¼‚ï¼‰
                days_diff = (today - latest_date).days
                if days_diff > 1:
                    return False
            except Exception:
                return False  # æ—¥æœŸè§£æå¤±è´¥

            return True  # æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡

        except Exception:
            return False  # ä»»ä½•å¼‚å¸¸éƒ½è®¤ä¸ºéœ€è¦é‡æ–°ä¸‹è½½

    def get_existing_coin_ids(self) -> Set[str]:
        """è·å–å·²å­˜åœ¨çš„å¸ç§ID"""
        existing_ids = set()
        if self.coins_dir.exists():
            for csv_file in self.coins_dir.glob("*.csv"):
                coin_id = csv_file.stem
                existing_ids.add(coin_id)
        return existing_ids

    def update_with_smart_strategy(
        self, target_native_coins: int = 510, max_search_range: int = 1000
    ):
        """
        æ™ºèƒ½æ›´æ–°ç­–ç•¥

        Args:
            target_native_coins: ç›®æ ‡åŸç”Ÿå¸ç§æ•°é‡
            max_search_range: æœ€å¤§æœç´¢èŒƒå›´
        """
        logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½é‡ä»·æ•°æ®æ›´æ–°")
        logger.info(f"ğŸ“‹ ç›®æ ‡: ç¡®ä¿è‡³å°‘ {target_native_coins} ä¸ªåŸç”Ÿå¸ç§æ•°æ®æœ€æ–°")
        logger.info(f"ğŸ” æœ€å¤§æœç´¢èŒƒå›´: {max_search_range} ä¸ªå¸ç§")
        logger.info("=" * 60)

        self.stats["start_time"] = datetime.now()

        try:
            # 1. è·å–ç°æœ‰å¸ç§ID
            existing_ids = self.get_existing_coin_ids()
            logger.info(f"ğŸ“‹ ç°æœ‰å¸ç§æ•°é‡: {len(existing_ids)}")

            # 2. æŒ‰å¸‚å€¼é¡ºåºè·å–å¸ç§å¹¶é€ä¸ªå¤„ç†
            native_coins_updated = 0
            search_range = min(
                max_search_range, target_native_coins * 2
            )  # å¼€å§‹æœç´¢èŒƒå›´

            while (
                native_coins_updated < target_native_coins
                and search_range <= max_search_range
            ):
                logger.info(f"ğŸ” æœç´¢å¸‚å€¼å‰ {search_range} åå¸ç§...")

                # è·å–å¸‚å€¼æ’åæ•°æ®
                all_coins = self.market_fetcher.get_top_coins(search_range)

                # æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªå¸ç§
                with tqdm(
                    total=len(all_coins),
                    desc="å¤„ç†å¸ç§æ•°æ®",
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                    ncols=120,
                    leave=False,
                ) as pbar:
                    for coin_info in all_coins:
                        coin_id = coin_info["id"]
                        coin_symbol = coin_info["symbol"].upper()

                        try:
                            # ä½¿ç”¨ç»Ÿä¸€åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»
                            classification_result = self.classifier.classify_coin(
                                coin_id
                            )

                            # ç¡®å®šå¸ç§ç±»å‹
                            if classification_result.is_stablecoin:
                                coin_type = "stable"
                            elif classification_result.is_wrapped_coin:
                                coin_type = "wrapped"
                            else:
                                coin_type = "native"

                            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                            # æ¯ä¸ªå¸ç§éƒ½ç›´æ¥ä¸‹è½½å…¨é‡æ•°æ®ï¼Œç®€å•ç›´æ¥
                            success, api_called = self.download_coin_data(coin_id)

                            if success:
                                # æ›´æ–°ç»Ÿè®¡
                                if coin_type == "native":
                                    native_coins_updated += 1
                                    self.stats["native_updated"] += 1
                                elif coin_type == "stable":
                                    self.stats["stable_updated"] += 1
                                elif coin_type == "wrapped":
                                    self.stats["wrapped_updated"] += 1

                                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¸ç§
                                if coin_id not in existing_ids:
                                    self.stats["new_coins"] += 1
                                    existing_ids.add(coin_id)

                                # åªåœ¨å®é™…è°ƒç”¨APIæ—¶è®¡æ•°
                                if api_called:
                                    self.stats["api_calls"] += 1

                                # APIé™æµå»¶è¿Ÿï¼ˆåªåœ¨å®é™…è°ƒç”¨APIæ—¶å»¶è¿Ÿï¼‰
                                if api_called:
                                    time.sleep(RATE_LIMIT_CONFIG["delay_seconds"])
                            else:
                                # å¤±è´¥çš„æƒ…å†µä¹Ÿè¦ç»Ÿè®¡APIè°ƒç”¨
                                if api_called:
                                    self.stats["api_calls"] += 1
                                self.stats["failed_updates"] += 1
                                self.errors.append(f"{coin_id}: ä¸‹è½½å¤±è´¥")

                            self.stats["total_processed"] += 1

                            # æ›´æ–°è¿›åº¦æ¡
                            pbar.set_postfix(
                                {
                                    "åŸç”Ÿå¸": native_coins_updated,
                                    "ç›®æ ‡": target_native_coins,
                                    "ç±»å‹": coin_type,
                                    "å½“å‰": coin_symbol[:10],
                                }
                            )
                            pbar.update(1)

                            # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡ï¼Œæå‰ç»“æŸ
                            if native_coins_updated >= target_native_coins:
                                logger.info(
                                    f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡ï¼æˆåŠŸå¤„ç† {native_coins_updated} ä¸ªåŸç”Ÿå¸ç§"
                                )
                                break

                        except Exception as e:
                            logger.error(f"å¤„ç† {coin_id} æ—¶å‡ºé”™: {e}")
                            self.errors.append(f"{coin_id}: {str(e)}")
                            self.stats["failed_updates"] += 1
                            pbar.update(1)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å¤§æœç´¢èŒƒå›´
                if native_coins_updated < target_native_coins:
                    if search_range >= max_search_range:
                        logger.warning(
                            f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æœç´¢èŒƒå›´ {max_search_range}ï¼Œä½†åªæ‰¾åˆ° {native_coins_updated} ä¸ªåŸç”Ÿå¸ç§"
                        )
                        break
                    else:
                        search_range = min(search_range + 200, max_search_range)
                        logger.info(f"ğŸ”„ æ‰©å¤§æœç´¢èŒƒå›´åˆ° {search_range}...")
                else:
                    break

            # æ›´æ–°å…ƒæ•°æ®
            logger.info("ğŸ’° æ›´æ–°ç¨³å®šå¸å’ŒåŒ…è£…å¸å…ƒæ•°æ®...")
            self.update_metadata()

            # éªŒè¯ç»“æœ
            if native_coins_updated >= target_native_coins:
                logger.info(
                    f"âœ… æˆåŠŸè¾¾æˆç›®æ ‡ï¼å¤„ç†äº† {native_coins_updated} ä¸ªåŸç”Ÿå¸ç§"
                )
            else:
                logger.warning(
                    f"âš ï¸ æœªå®Œå…¨è¾¾æˆç›®æ ‡ï¼Œåªå¤„ç†äº† {native_coins_updated}/{target_native_coins} ä¸ªåŸç”Ÿå¸ç§"
                )

        except Exception as e:
            logger.error(f"æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            self.errors.append(f"æ›´æ–°å¼‚å¸¸: {str(e)}")

        finally:
            self.stats["end_time"] = datetime.now()
            duration = self.stats["end_time"] - self.stats["start_time"]

            # ç”ŸæˆæŠ¥å‘Š
            self.generate_final_report(duration)

    def update_metadata(self):
        """æ›´æ–°ç¨³å®šå¸å’ŒåŒ…è£…å¸å…ƒæ•°æ®"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€åˆ†ç±»å™¨æ›´æ–°å…ƒæ•°æ®
            classifier = UnifiedClassifier()

            # è·å–æ‰€æœ‰å¸ç§æ•°æ®
            from ..downloaders.batch_downloader import create_batch_downloader

            downloader = create_batch_downloader()
            metadata_dir = Path(downloader.data_dir) / "metadata" / "coin_metadata"

            if not metadata_dir.exists():
                logger.warning("âŒ å…ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨")
                return

            coin_ids = [f.stem for f in metadata_dir.glob("*.json")]
            if not coin_ids:
                logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•å¸ç§å…ƒæ•°æ®")
                return

            logger.info(f"ğŸ” æ­£åœ¨åˆ†æ {len(coin_ids)} ä¸ªå¸ç§...")

            # æ‰¹é‡åˆ†ç±»
            classification_results = classifier.classify_coins_batch(coin_ids)

            # å¯¼å‡ºç¨³å®šå¸åˆ—è¡¨
            stablecoins = [
                result
                for result in classification_results.values()
                if result.is_stablecoin
            ]
            stable_file = self.metadata_dir / "stablecoins.csv"

            with open(stable_file, "w", encoding="utf-8") as f:
                f.write("coin_id,symbol,name\n")
                for result in stablecoins:
                    f.write(
                        f"{result.coin_id},{result.symbol or ''},{result.name or ''}\n"
                    )

            logger.info(f"âœ… ç¨³å®šå¸åˆ—è¡¨å·²å¯¼å‡ºåˆ°: {stable_file}")
            logger.info(f"   å…±å¯¼å‡º {len(stablecoins)} ä¸ªç¨³å®šå¸")

            # å¯¼å‡ºåŒ…è£…å¸åˆ—è¡¨
            wrapped_coins = [
                result
                for result in classification_results.values()
                if result.is_wrapped_coin
            ]
            wrapped_file = self.metadata_dir / "wrapped_coins.csv"

            with open(wrapped_file, "w", encoding="utf-8") as f:
                f.write("coin_id,symbol,name\n")
                for result in wrapped_coins:
                    f.write(
                        f"{result.coin_id},{result.symbol or ''},{result.name or ''}\n"
                    )

            logger.info(f"âœ… åŒ…è£…å¸åˆ—è¡¨å·²å¯¼å‡ºåˆ°: {wrapped_file}")
            logger.info(f"   å…±å¯¼å‡º {len(wrapped_coins)} ä¸ªåŒ…è£…å¸")

            logger.info("âœ… å…ƒæ•°æ®æ›´æ–°å®Œæˆ")
        except Exception as e:
            logger.error(f"æ›´æ–°å…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            self.errors.append(f"å…ƒæ•°æ®æ›´æ–°é”™è¯¯: {str(e)}")

    def generate_final_report(self, duration):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report = f"""
ğŸ” æ™ºèƒ½é‡ä»·æ•°æ®æ›´æ–°æŠ¥å‘Š
============================================================
ğŸ“Š å¤„ç†ç»Ÿè®¡:
   - æ€»å¤„ç†å¸ç§æ•°: {self.stats['total_processed']}
   - åŸç”Ÿå¸æ›´æ–°æ•°: {self.stats['native_updated']}
   - ç¨³å®šå¸æ›´æ–°æ•°: {self.stats['stable_updated']}
   - åŒ…è£…å¸æ›´æ–°æ•°: {self.stats['wrapped_updated']}
   - æ–°å¸ç§æ•°: {self.stats['new_coins']}
   - å¤±è´¥æ›´æ–°æ•°: {self.stats['failed_updates']}

âš¡ æ€§èƒ½ç»Ÿè®¡:
   - APIè°ƒç”¨æ¬¡æ•°: {self.stats['api_calls']}
   - æ€»è€—æ—¶: {duration}

ğŸ• æ—¶é—´ä¿¡æ¯:
   - å¼€å§‹æ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}
   - ç»“æŸæ—¶é—´: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}

{'âœ… æ— é”™è¯¯' if not self.errors else f'âŒ é”™è¯¯åˆ—è¡¨:'}
{chr(10).join(f'   - {error}' for error in self.errors[:10]) if self.errors else ''}
"""

        logger.info(report)

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = (
            Path("logs")
            / f"smart_update_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        )
        report_file.write_text(report, encoding="utf-8")

        logger.info(f"ğŸ“‹ æ›´æ–°æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        logger.info("ğŸ‰ æ™ºèƒ½é‡ä»·æ•°æ®æ›´æ–°æµç¨‹å®Œæˆï¼")
